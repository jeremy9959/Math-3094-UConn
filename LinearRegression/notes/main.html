<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Linear Regression</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
  <!-- pandoc-eqnos: equation style -->
  <style>
    .eqnos { display: inline-block; position: relative; width: 100%; }
    .eqnos br { display: none; }
    .eqnos-number { position: absolute; right: 0em; top: 50%; line-height: 0; }
  </style>
</head>
<body>

<h1 id="linear-regression"><span class="header-section-number">1</span> Linear Regression</h1>
<h2 id="sec:Intro"><span class="header-section-number">1.1</span> Introduction</h2>
<p>Suppose that we are trying to study two quantities <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> that we suspect are related – at least approximately – by a linear equation <span class="math inline">\(y=ax+b\)</span>. Sometimes this linear relationship is predicted by theoretical considerations, and sometimes it is just an empirical hypothesis.</p>
<p>For example, if we are trying to determine the velocity of an object travelling towards us at constant speed, and we measure measure the distances <span class="math inline">\(d_1, d_2, \ldots, d_n\)</span> between us and the object at a series of times <span class="math inline">\(t_1, t_2, \ldots, t_n\)</span>, then since “distance equals rate times time” we have a theoretical foundation for the assumption that <span class="math inline">\(d=rt+b\)</span> for some constants <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span>. On the other hand, because of unavoidable experimental errors, we can’t expect that this relationship will hold exactly for the observed data; instead, we likely get a graph like that shown in fig. <a href="#fig:dvt">1</a>.</p>
<div id="fig:dvt" class="fignos">
<figure>
<img src="../img/mpg-vs-displacement.png" alt="Figure 1: Physics Experiment" style="width:3in" /><figcaption><span>Figure 1:</span> Physics Experiment</figcaption>
</figure>
</div>
<p>On the other hand, we might look at a graph such as fig. <a href="#fig:mpg-vs-displacement">2</a>, which plots the gas mileage of various car models against their engine size (displacement), and observe a general trend in which bigger engines get lower mileage. In this situation we could ask for the best line of the form <span class="math inline">\(y=mx+b\)</span> that captures this relationship and use that to make general conclusions without necessarily having an underlying theory.</p>
<div id="fig:mpg-vs-displacement" class="fignos">
<figure>
<img src="../img/mpg-vs-displacement.png" alt="Figure 2: MPG vs Displacement ([1])" style="width:3in" /><figcaption><span>Figure 2:</span> MPG vs Displacement (<span class="citation" data-cites="irvine">[<a href="#ref-irvine">1</a>]</span>)</figcaption>
</figure>
</div>
<h2 id="sec:Calculus"><span class="header-section-number">1.2</span> Least Squares (via Calculus)</h2>
<p>In either of the two cases above, the question we face is to determine the line <span class="math inline">\(y=mx+b\)</span> that “best fits” the data <span class="math inline">\(\{(x_i,y_i)_{i=1}^{N}\}\)</span>. The classic approach is to determine the equation of a line <span class="math inline">\(y=mx+b\)</span> that minimizes the “mean squared error”:</p>
<p><span class="math display">\[
MSE(m,b) = \frac{1}{N}\sum_{i=1}^{n} (y_i-mx_i-b)^2
\]</span></p>
<p>It’s worth emphasizing that the <span class="math inline">\(MSE\)</span> is a function of two variables – the slope <span class="math inline">\(m\)</span> and the intercept <span class="math inline">\(b\)</span> – and that the data points <span class="math inline">\(\{(x_i,y_i)\}\)</span> are constants for these purposes. Furthermore, it’s a quadratic function in those two variables. Since our goal is to find <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> that minimize the <span class="math inline">\(MSE\)</span>, we have a Calculus problem that we can solve by taking partial derivatives and setting them to zero.</p>
<p>To simplify the notation, let’s abbreviate <span class="math inline">\(MSE\)</span> by <span class="math inline">\(E\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\frac{\partial E}{\partial m} &amp;= \frac{1}{N}\sum_{1}^{N}-2x_i(y_i-mx_i-b) \\
\frac{\partial E}{\partial b} &amp;= \frac{1}{N}\sum_{1}^{N}-2(y_i-mx_i-b) \\
\end{aligned}
\]</span></p>
<p>We set these two partial derivatives to zero, so we can drop the <span class="math inline">\(-2\)</span> and regroup the sums to obtain two equations in two unknowns (we keep the <span class="math inline">\(\frac{1}{N}\)</span> because it is illuminating in the final result):</p>
<p><span id="eq:LS" class="eqnos"><span class="math display">\[
\begin{aligned}
\frac{1}{N}(\sum_{i=1}^{N} x_i^2)m &amp;+&amp; \frac{1}{N}(\sum_{i=1}^{N} x_i)b &amp;=&amp; \frac{1}{N}\sum_{i=1}^{N} x_i y_i \\
\frac{1}{N}(\sum_{i=1}^{N} x_i)m &amp;+&amp; b &amp;=&amp; \frac{1}{N}\sum_{i=1}^{N} y_{i} \\
\end{aligned}
\]</span><span class="eqnos-number">(1)</span></span></p>
<p>In these equations, notice that <span class="math inline">\(\frac{1}{N}\sum_{i=1}^{N} x_i\)</span> is the average (or mean) value of the <span class="math inline">\(x_i\)</span>. Let’s call this <span class="math inline">\(\overline{x}\)</span>. Similarly, <span class="math inline">\(\frac{1}{N}\sum_{i=1}^{N} y_{i}\)</span> is the mean of the <span class="math inline">\(y_i\)</span>, and we’ll call it <span class="math inline">\(\overline{y}\)</span>. If we further simplify the notation and write <span class="math inline">\(S_{xx}\)</span> for <span class="math inline">\(\frac{1}{N}\sum_{i=1}^{N} x_i^2\)</span> and <span class="math inline">\(S_{xy}\)</span> for <span class="math inline">\(\frac{1}{N}\sum_{i=1}^{N}x_iy_i\)</span> then we can write down a solution to this system using Cramer’s rule:</p>
<p><span id="eq:LSAnswer" class="eqnos"><span class="math display">\[
\begin{aligned}
m &amp;= \frac{S_{xy}-\overline{x}\overline{y}}{S_{xx}-\overline{x}^2} \\
b &amp;= \frac{S_{xx}\overline{y}-S_{xy}\overline{x}}{S_{xx}-\overline{x}^2} \\
\end{aligned}
\]</span><span class="eqnos-number">(2)</span></span></p>
<p>where we must have <span class="math inline">\(S_{xx}-\overline{x}^2\not=0\)</span>.</p>
<h3 id="sec:CalcExercises"><span class="header-section-number">1.2.1</span> Exercises</h3>
<ol type="1">
<li><p>Verify that eq. <a href="#eq:LSAnswer">2</a> is in fact the solution to the system in eq. <a href="#eq:LS">1</a>.</p></li>
<li><p>Suppose that <span class="math inline">\(S_{xx}-\overline{x}^2=0\)</span>. What does that mean about the <span class="math inline">\(x_i\)</span>? Does it make sense that the problem of finding the “line of best fit” fails in this case?</p></li>
</ol>
<h2 id="sec:LinAlg"><span class="header-section-number">1.3</span> Least Squares (via Geometry)</h2>
<p>In our discussion above, we thought about our data as consisting of <span class="math inline">\(N\)</span> pairs <span class="math inline">\((x_i,y_i)\)</span> corresponding to <span class="math inline">\(n\)</span> points in the <span class="math inline">\(xy\)</span>-plane <span class="math inline">\(\mathbf{R}^2\)</span>. Now let’s turn that picture “on its side”, and instead think of our data as consisting of <em>two</em> points in <span class="math inline">\(\mathbf{R}^{n}\)</span>:</p>
<p><span class="math display">\[
X=\left[\begin{matrix} x_1\cr x_2\cr \vdots\cr x_n\end{matrix}\right] \mathrm{\ and\ }
Y = \left[\begin{matrix} y_1\cr y_2\cr \vdots\cr y_n\end{matrix}\right]
\]</span></p>
<p>Let’s also introduce one other vector</p>
<p><span class="math display">\[
E = \left[\begin{matrix} 1 \cr 1 \cr \vdots \cr 1\end{matrix}\right].
\]</span></p>
<p>First, let’s assume that <span class="math inline">\(E\)</span> and <span class="math inline">\(X\)</span> are linearly independent. If not, then <span class="math inline">\(X\)</span> is a constant vector (why?) which we already know is a problem from section <a href="#sec:Calculus">1.2</a>, Exercise 2. Therefore <span class="math inline">\(E\)</span> and <span class="math inline">\(X\)</span> span a plane in <span class="math inline">\(\mathbf{R}^{n}\)</span>.</p>
<div id="fig:perp" class="fignos">
<figure>
<img src="../img/mpg-vs-displacement.png" alt="Figure 3: Distance to A Plane" style="width:3in" /><figcaption><span>Figure 3:</span> Distance to A Plane</figcaption>
</figure>
</div>
<p>Now if our data points <span class="math inline">\((x_i,y_i)\)</span> all <em>did</em> lie on a line <span class="math inline">\(y=mx+b\)</span>, then the three vectors <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, and <span class="math inline">\(E\)</span> would be linearly dependent:</p>
<p><span class="math display">\[
Y = mX + bE.
\]</span></p>
<p>Since our data is only approximately linear, that’s not the case. So instead we look for an approximate solution. One way to phrase that is to ask:</p>
<p><em>What is the point <span class="math inline">\(\hat{Y}\)</span> in the plane <span class="math inline">\(H\)</span> spanned by <span class="math inline">\(X\)</span> and <span class="math inline">\(E\)</span> in <span class="math inline">\(\mathbf{R}^{n}\)</span> which is closest to <span class="math inline">\(Y\)</span>?</em></p>
<p>If we knew this point <span class="math inline">\(\hat{Y}\)</span>, then since it lies in <span class="math inline">\(H\)</span> we would have <span class="math inline">\(\hat{Y}=mX+bE\)</span> and the coefficients <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> would be a candidate for defining a line of best fit <span class="math inline">\(y=mx+b\)</span>. Finding the point in a plane closest to another point in <span class="math inline">\(\mathbf{R}^{n}\)</span> is a geometry problem that we can solve.</p>
<p><strong>Proposition:</strong> The point <span class="math inline">\(\hat{Y}\)</span> in the plane spanned by <span class="math inline">\(X\)</span> and <span class="math inline">\(E\)</span> is the point such that the vector <span class="math inline">\(Y-\hat{Y}\)</span> is perpendicular to <span class="math inline">\(H\)</span>.</p>
<p><strong>Proof:</strong> See fig. <a href="#fig:perp">3</a> for an illustration – perhaps you are already convinced by this, but let’s be careful. <span class="math inline">\(\hat{Y}=mX+bE\)</span> such that <span class="math display">\[
D = \|Y-\hat{Y}\|^2 = \|Y-mX-bE\|^2
\]</span> is minimal. Using some vector calculus, we have <span class="math display">\[
\frac{\partial D}{\partial m} = \frac{\partial}{\partial m} (Y-mX-bE)\cdot (Y-mX-bE) = -2(Y-mX-bE)\cdot X
\]</span> and <span class="math display">\[
\frac{\partial D}{\partial b} = \frac{\partial}{\partial b} (Y-mX-bE)\cdot (Y-mX-bE) = -2(Y-mX-bE)\cdot E.
\]</span></p>
<p>So both derivatives are zero exactly when <span class="math inline">\(\hat{Y}=(Y-mX-bE)\)</span> is orthogonal to both <span class="math inline">\(X\)</span> and <span class="math inline">\(E\)</span>, and therefore every vector in <span class="math inline">\(H\)</span>.</p>
<p>We also obtain equations for <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> just as in our first look at this problem.</p>
<p><span id="eq:LSAnswer2" class="eqnos"><span class="math display">\[
\begin{aligned}
m(X\cdot E) &amp;+ b(E\cdot E) &amp;= (Y\cdot E) \cr
m(X\cdot X) &amp;+ b(E\cdot X) &amp;= (Y\cdot X) \cr
\end{aligned}
\]</span><span class="eqnos-number">(3)</span></span></p>
<p>We leave it is an exercise below to check that these are the same equations that we obtained in eq. <a href="#eq:LSAnswer">2</a>.</p>
<h3 id="exercises"><span class="header-section-number">1.3.1</span> Exercises</h3>
<ol type="1">
<li>Verify that eq. <a href="#eq:LSAnswer">2</a> and eq. <a href="#eq:LSAnswer2">3</a> are equivalent.</li>
</ol>
<h2 id="sec:Multivariate-calculus"><span class="header-section-number">1.4</span> The Multivariate Case (Calculus)</h2>
<p>Having worked through the problem of finding a “line of best fit” from two points of view, let’s look at a more general problem. We looked above at a scatterplot showing the relationship between gas mileage and engine size (displacement). There are other factors that might contribute to gas mileage that we want to consider as well – for example: - a car that is heavy compared to its engine size may get worse mileage - a sports car with a drive train that gives fast acceleration as compared to a car with a transmission designed for long trips may have different mileage for the same engine size.</p>
<p>Suppose we wish to use engine displacement, vehicle weight, and acceleration all together to predict mileage. Instead of looking points <span class="math inline">\((x_i,y_i)\)</span> where <span class="math inline">\(x_i\)</span> is the displacement of the <span class="math inline">\(i^{th}\)</span> car model and we try to predict a value <span class="math inline">\(y\)</span> from a corresponding <span class="math inline">\(x\)</span> as <span class="math inline">\(y=mx+b\)</span> – let’s look at a situation in which our measured value <span class="math inline">\(y\)</span> depends on multiple variables – say displacement <span class="math inline">\(d\)</span>, weight <span class="math inline">\(w\)</span>, and acceleration <span class="math inline">\(a\)</span> with <span class="math inline">\(k=3\)</span> – and we are trying to find the best linear equation</p>
<p><span id="eq:multivariate" class="eqnos"><span class="math display">\[
y=m_1 d + m_2 w + m_3 a +b
\]</span><span class="eqnos-number">(4)</span></span></p>
<p>But to handle this situation more generally we need to adopt a convention that will allow us to use indexed variables instead of <span class="math inline">\(d\)</span>, <span class="math inline">\(w\)</span>, and <span class="math inline">\(a\)</span>. We will use the <em>tidy</em> data convention.</p>
<p><strong>Tidy Data:</strong> A dataset is tidy if it consists of values <span class="math inline">\(x_{ij}\)</span> for <span class="math inline">\(i=1,\ldots,N\)</span> and <span class="math inline">\(j=1,\ldots, k\)</span> so that:</p>
<ul>
<li>the row index corresponds to a <em>sample</em> – a set of measurements from a single event or item;</li>
<li>the column index corresponds to a <em>feature</em> – a particular property measured for all of the events or items.</li>
</ul>
<p>In our case,</p>
<ul>
<li>the <em>samples</em> are the different types of car models,</li>
<li>the <em>features</em> are the properties of those car models.</li>
</ul>
<p>For us, <span class="math inline">\(N\)</span> is the number of different types of cars, and <span class="math inline">\(k\)</span> is the number of properties we are considering. Since we are looking at displacement, weight, and acceleration, we have <span class="math inline">\(k=3\)</span>.</p>
<p>So the “independent variables” for a set of data that consists of <span class="math inline">\(N\)</span> samples, and <span class="math inline">\(k\)</span> measurements for each sample, can be represented by a <span class="math inline">\(N\times k\)</span> matrix</p>
<p><span class="math display">\[
X = \left(\begin{matrix}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1k} \\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{N1} &amp; x_{k2} &amp; \cdots &amp; x_{Nk} \\
\end{matrix}\right)
\]</span></p>
<p>and the measured dependent variables <span class="math inline">\(Y\)</span> are a column vector <span class="math display">\[
Y = \left[\begin{matrix} y_1 \\ y_2 \\ \vdots \\ y_N\end{matrix}\right].
\]</span></p>
<p>If <span class="math inline">\(m_1,\ldots, m_k\)</span> are “slopes” associated with these properties in eq. <a href="#eq:multivariate">4</a>, and <span class="math inline">\(b\)</span> is the “intercept”, then the predicted value <span class="math inline">\(\hat{Y}\)</span> is given by a matrix equation</p>
<p><span class="math display">\[
\hat{Y} = X\left[\begin{matrix} m_1 \\ m_2 \\ \cdots \\ m_k\end{matrix}\right]+\left[\begin{matrix} 1 \\ 1 \\ \cdots \\ 1\end{matrix}\right]b
\]</span></p>
<p>and our goal is to choose these parameters <span class="math inline">\(m_i\)</span> and <span class="math inline">\(b\)</span> to make the mean squared error:</p>
<p><span class="math display">\[
MSE(m_1,\ldots, m_k,b) = \|Y-\hat{Y}\|^2 = \sum_{i=1}^{N} (y_i - \sum_{j=1}^{k}  x_{ij}m_j -b )^2.
\]</span></p>
<p>Here we are summing over the <span class="math inline">\(N\)</span> different car models, and for each model taking the squared difference between the true mileage <span class="math inline">\(y_i\)</span> and the “predicted” mileage <span class="math inline">\(\sum_{j=1}^{k} x_{ij}m_j +b\)</span>. We wish to minimize this MSE.</p>
<p>Let’s make one more simplification. The intercept variable <span class="math inline">\(b\)</span> is annoying because it requires separate treatment from the <span class="math inline">\(m_i\)</span>. But we can use a trick to eliminate the need for special treatment. Let’s add a new feature to our data matrix (a new column) that has the constant value <span class="math inline">\(1\)</span>.</p>
<p><span class="math display">\[
X = \left(\begin{matrix}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1k} &amp; 1\\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2k} &amp; 1\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; 1\\
x_{N1} &amp; x_{k2} &amp; \cdots &amp; x_{Nk} &amp; 1\\
\end{matrix}\right)
\]</span></p>
<p>Now our data matrix <span class="math inline">\(X\)</span> is <span class="math inline">\(N\times(k+1)\)</span> and we can put our “intercept” <span class="math inline">\(b=m_{k+1}\)</span> into our vector of “slopes” <span class="math inline">\(m_1, \ldots, m_k,m_{k+1}\)</span>:</p>
<p><span class="math display">\[
\hat{Y} = X\left[\begin{matrix} m_1 \\ m_2 \\ \cdots \\ m_k \\ m_{k+1}\end{matrix}\right].
\]</span></p>
<p>and our MSE becomes</p>
<p><span class="math display">\[
MSE(M) = \|Y - XM\|^2
\]</span></p>
<p>where</p>
<p><span class="math display">\[
M=\left[\begin{matrix} m_1 \\ m_2 \\ \cdots \\ m_k \\ m_{k+1}\end{matrix}\right].
\]</span></p>
<p>The Calculus approach to minimizing the <span class="math inline">\(MSE\)</span> is to take its partial derivatives with respect to the <span class="math inline">\(m_{i}\)</span> and set them to zero. Let’s first work out the derivatives in a nice form for later.</p>
<p><strong>Proposition:</strong> The gradient of <span class="math inline">\(MSE(M)=E\)</span> is given by</p>
<p><span id="eq:gradient" class="eqnos"><span class="math display">\[
\nabla E = \left[\begin{matrix} \frac{\partial}{\partial m_1}E \\ \frac{\partial}{\partial m_2}E \\ \vdots \\ \frac{\partial}{\partial m_{k+1}}E\end{matrix}\right] =  -2 X^{\intercal}Y + 2 X^{\intercal}XM
\]</span><span class="eqnos-number">(5)</span></span></p>
<p>where <span class="math inline">\(X^{\intercal}\)</span> is the transpose of <span class="math inline">\(X\)</span>.</p>
<p><strong>Proof:</strong> First, remember that the <span class="math inline">\(ij\)</span> entry of <span class="math inline">\(X^{\intercal}\)</span> is the <span class="math inline">\(ji\)</span> entry of <span class="math inline">\(X\)</span>. Also, we will use the notation <span class="math inline">\(X[j,:]\)</span> to mean the <span class="math inline">\(j^{th}\)</span> row of <span class="math inline">\(X\)</span> and <span class="math inline">\(X[:,i]\)</span> to mean the <span class="math inline">\(i^{th}\)</span> column of <span class="math inline">\(X\)</span>. (This is copied from the Python programming language; the ‘:’ means that index runs over all possibilities).</p>
<p>Since <span class="math display">\[
E = \sum_{j=1}^{N} (Y_j-\sum_{s=1}^{k+1} X_{js}M_{s})^2
\]</span> we compute: <span id="eq:gradient" class="eqnos"><span class="math display">\[\begin{aligned}
\frac{\partial}{\partial m_t}E &amp;= -2\sum_{j=1}^{N} X_{jt}(Y_{j}-\sum_{s=1}^{k+1} X_{js}M_{s}) \\
&amp;= -2(\sum_{j=1}^{N} Y_{j}X_{jt} - \sum_{j=1}^{N}\sum_{s=1}^{k+1} X_{jt}X_{js}M_{s}) \\
&amp;= -2(\sum_{j=1}^{N} X^{\intercal}_{tj}Y_{j} -\sum_{j=1}^{N}\sum_{s=1}^{k+1} X^{\intercal}_{tj}X_{js}M_{s}) \\
&amp;= -2(X^{\intercal}[t,:]Y - \sum_{s=1}^{k+1}\sum_{j=1}^{N} X^{\intercal}_{tj}X_{js}M_{s}) \\
&amp;= -2(X^{\intercal}[t,:]Y - \sum_{s=1}^{k+1} (X^{\intercal}X)_{ts}M_{s}) \\
&amp;= -2(X^{\intercal}[t,:]Y - (X^{\intercal}X)[t,:]M)\\
\end{aligned}\]</span><span class="eqnos-number">(6)</span></span></p>
<p>Stacking up the different rows to make <span class="math inline">\(E\)</span> yields the desired formula.</p>
<p><strong>Proposition:</strong> Assume that <span class="math inline">\(D=X^{\intercal}X\)</span> is invertible (notice that it is a <span class="math inline">\((k+1)\times(k+1)\)</span> square matrix so this makes sense). The solution <span class="math inline">\(M\)</span> to the multivariate least squares problem is <span class="math display">\[
M = D^{-1}X^{\intercal}Y
\]</span> and the “predicted value” <span class="math inline">\(\hat{Y}\)</span> for <span class="math inline">\(Y\)</span> is <span id="eq:projection" class="eqnos"><span class="math display">\[
\hat{Y} = XD^{-1}X^{\intercal}Y.
\]</span><span class="eqnos-number">(7)</span></span></p>
<h2 id="the-multivariate-case-geometry"><span class="header-section-number">1.5</span> The Multivariate Case (Geometry)</h2>
<p>Let’s look more closely at the equation obtained by setting the gradient of the error, eq. <a href="#eq:gradient">6</a>, to zero. Remember that <span class="math inline">\(M\)</span> is the unknown vector in this equation, everything else is known:</p>
<p><span class="math display">\[
X^{\intercal}Y = X^{\intercal}XM
\]</span></p>
<p>Here is how to think about this:</p>
<ol type="1">
<li><p>As <span class="math inline">\(M\)</span> varies, the <span class="math inline">\(N\times 1\)</span> matrix <span class="math inline">\(XM\)</span> varies over the space spanned by the columns of the matrix <span class="math inline">\(X\)</span>. So as <span class="math inline">\(M\)</span> varies <span class="math inline">\(XM\)</span> is a general element of the subspace <span class="math inline">\(H\)</span> of <span class="math inline">\(R^{N}\)</span> spanned by the <span class="math inline">\(k+1\)</span> columns of <span class="math inline">\(X\)</span>.</p></li>
<li><p>The product <span class="math inline">\(X^{\intercal}XM\)</span> is a <span class="math inline">\((k+1)\times 1\)</span> matrix. Each entry is the dot product of the general element of <span class="math inline">\(H\)</span> with one of the <span class="math inline">\(k+1\)</span> basis vectors of <span class="math inline">\(H\)</span>.</p></li>
<li><p>The product <span class="math inline">\(X^{\intercal}Y\)</span> is a <span class="math inline">\((k+1)\times 1\)</span> matrix whose entries are the dot product of the basis vectors of <span class="math inline">\(H\)</span> with <span class="math inline">\(Y\)</span>.</p></li>
</ol>
<p>Therefore, this equation asks for us to find <span class="math inline">\(M\)</span> so that the vector <span class="math inline">\(XM\)</span> in <span class="math inline">\(H\)</span> has the same dot products with the basis vectors of <span class="math inline">\(H\)</span> as <span class="math inline">\(Y\)</span> does. The condition</p>
<p><span class="math display">\[
X^{\intercal}\cdot (Y-XM)=0
\]</span></p>
<p>says that <span class="math inline">\(Y-XM\)</span> is orthogonal to <span class="math inline">\(H\)</span>. This argument establishes the following proposition.</p>
<p><strong>Proposition:</strong> Just as in the simple one-dimensional case, the predicted value <span class="math inline">\(\hat{Y}\)</span> of the least squares problem is the point in <span class="math inline">\(H\)</span> closest to <span class="math inline">\(Y\)</span> – or in other words the point <span class="math inline">\(\hat{Y}\)</span> in <span class="math inline">\(H\)</span> such that <span class="math inline">\(Y-\hat{Y}\)</span> is perpendicular to <span class="math inline">\(H\)</span>.</p>
<h3 id="orthogonal-projection"><span class="header-section-number">1.5.1</span> Orthogonal Projection</h3>
<p>Recall that we introduced the notation <span class="math inline">\(D=X^{\intercal}X\)</span>, and let’s assume, for now, that <span class="math inline">\(D\)</span> is an invertible matrix. We have the formula (see eq. <a href="#eq:projection">7</a>): <span class="math display">\[
\hat{Y} = XD^{-1}X^{\intercal}Y.
\]</span> <strong>Proposition:</strong> The matrix <span class="math inline">\(P=XD^{-1}X^{\intercal}\)</span> is an <span class="math inline">\(N\times N\)</span> matrix called the orthogonal projection operator onto the subspace <span class="math inline">\(H\)</span> spanned by the columns of <span class="math inline">\(X\)</span>. It has the following properties:</p>
<ul>
<li><span class="math inline">\(PY\)</span> belongs to the subspace <span class="math inline">\(H\)</span> for any <span class="math inline">\(Y\in\mathbf{R}^{N}\)</span>.</li>
<li><span class="math inline">\((Y-PY)\)</span> is orthogonal to <span class="math inline">\(H\)</span>.</li>
<li><span class="math inline">\(P*P = P\)</span>.</li>
</ul>
<p><strong>Proof:</strong> First of all, <span class="math inline">\(PY=XD^{-1}X^{\intercal}Y\)</span> so <span class="math inline">\(PY\)</span> is a linear combination of the columns of <span class="math inline">\(X\)</span> and is therefore an element of <span class="math inline">\(H\)</span>. Next, we can compute the dot product of <span class="math inline">\(PY\)</span> against a basis of <span class="math inline">\(H\)</span> by computing</p>
<p><span class="math display">\[
X^{\intercal}PY = X^{\intercal}XD^{-1}X^{\intercal}Y = X^{\intercal}Y
\]</span></p>
<p>since <span class="math inline">\(X^{\intercal}X=D\)</span>. This equation means that <span class="math inline">\(X^{\intercal}(Y-PY)=0\)</span> which tells us that <span class="math inline">\(Y-PY\)</span> has dot product zero with a basis for <span class="math inline">\(H\)</span>. Finally,</p>
<p><span class="math display">\[
PP = XD^{-1}X^{\intercal}XD^{-1}X^{\intercal} = XD^{-1}X^{\intercal}=P.
\]</span></p>
<p>It should be clear from the above discussion that the matrix <span class="math inline">\(D=X^{\intercal}X\)</span> plays an important role in the study of this problem. In particular it must be invertible or our analysis above breaks down. In the next section we will look more closely at this matrix and what information it encodes about our data.</p>
<h3 id="variance-covariance-and-the-covariance-matrix"><span class="header-section-number">1.5.2</span> Variance, Covariance and the Covariance Matrix</h3>
<p>Recall from last section that the matrix <span class="math inline">\(D=X^{\intercal}X\)</span> is of central importance to the study of the multivariate least squares problem. Let’s look at it more closely.</p>
<p><strong>Lemma:</strong> The <span class="math inline">\(i,j\)</span> entry of <span class="math inline">\(D\)</span> is the dot product <span class="math display">\[
D_{ij}=X[:,i]\cdot X[:,j]
\]</span> of the <span class="math inline">\(i^{th}\)</span> and <span class="math inline">\(j^{th}\)</span> columns of <span class="math inline">\(X\)</span>.</p>
<p><strong>Proof:</strong> In the matrix multiplication <span class="math inline">\(X^{\intercal}X\)</span>, the <span class="math inline">\(i^{th}\)</span> row of <span class="math inline">\(X^{\intercal}\)</span> gets “dotted” with the <span class="math inline">\(j^{th}\)</span> column of <span class="math inline">\(X\)</span> to product the <span class="math inline">\(i,j\)</span> entry. But the <span class="math inline">\(i^{th}\)</span> row of <span class="math inline">\(X^{\intercal}\)</span> is the <span class="math inline">\(i^{th}\)</span> column of <span class="math inline">\(X\)</span>, as asserted in the statement of the lemma.</p>
<p>A crucial point in our construction above relied on the matrix <span class="math inline">\(D\)</span> being invertible. The following Lemma shows that <span class="math inline">\(D\)</span> fails to be invertible only when the different features (the columns of <span class="math inline">\(X\)</span>) are linearly dependent.</p>
<p><strong>Lemma:</strong> <span class="math inline">\(D\)</span> is not invertible if and only if the columns of <span class="math inline">\(X\)</span> are linearly dependent.</p>
<p><strong>Proof:</strong> If the columns of <span class="math inline">\(X\)</span> are linearly dependent, then there is a nonzero vector <span class="math inline">\(m\)</span> so that <span class="math inline">\(Xm=0\)</span>. In that case clearly <span class="math inline">\(Dm=X^{\intercal}Xm=0\)</span> so <span class="math inline">\(D\)</span> is not invertible. Suppose <span class="math inline">\(D\)</span> is not invertible. Then there is a nonzero vector <span class="math inline">\(m\)</span> with <span class="math inline">\(Dm=X^{\intercal}Xm=0\)</span>. This means that the vector <span class="math inline">\(Xm\)</span> is orthogonal to all of the columns of <span class="math inline">\(X\)</span>. That in turn means that <span class="math inline">\(Xm=0\)</span> so the columns of <span class="math inline">\(X\)</span> are linearly dependent.</p>
<p>In fact, the matrix <span class="math inline">\(D\)</span> captures some important statistical measures of our data, but to see this clearly we need to make a slight change of basis. First recall that <span class="math inline">\(X[:,k+1]\)</span> is our column of all <span class="math inline">\(1\)</span>, added to handle the intercept. As a result, the dot product <span class="math inline">\(X[:,i]\cdot X[:,k+1]\)</span> is the sum of the entries in the <span class="math inline">\(i^{th}\)</span> column, and so if we let <span class="math inline">\(\mu_{i}\)</span> denote the average value of the entries in column <span class="math inline">\(i\)</span>, we have <span class="math display">\[
\mu_{i} = \frac{1}{N}(X[:,i]\cdot X[:,k+1])
\]</span></p>
<p>Now change the matrix <span class="math inline">\(X\)</span> by elementary column operations to obtain a new data matrix <span class="math inline">\(X_{0}\)</span> by setting <span class="math display">\[
X_{0}[:,i] = X[:,i]-\frac{1}{N}(X[:,i]\cdot X[:,k+1])X[:,k+1] = X[:,i]-\mu_{i}X[:,k+1]
\]</span> for <span class="math inline">\(i=1,\ldots, k\)</span>.</p>
<p>In terms of the original data, we are changing the measurement scale of the data so that each feature has average value zero, and the subspace <span class="math inline">\(H\)</span> spanned by the columns of <span class="math inline">\(X_{0}\)</span> is the same as that spanned by the columns of <span class="math inline">\(X\)</span>. Using <span class="math inline">\(X_{0}\)</span> instead of <span class="math inline">\(X\)</span> for our least squares problem, we get</p>
<p><span class="math display">\[
\hat{Y} = X_{0}D_{0}^{-1}X_{0}^{\intercal}Y
\]</span></p>
<p>and</p>
<p><span class="math display">\[
M_{0} = D_{0}^{-1}X_{0}^{\intercal}Y
\]</span></p>
<p>where <span class="math inline">\(D_{0}=X_{0}^{\intercal}X_{0}.\)</span></p>
<p><strong>Proposition:</strong> The matrix <span class="math inline">\(D_{0}\)</span> has a block form. Its upper left block is <span class="math inline">\(k\times k\)</span> symmetric block with entries <span class="math display">\[
(D_{0})_{ij} = (X[:,i]-\mu_{i}X[:,k+1])\cdot(X[:,j]-\mu_{j}X[:,k+1]).
\]</span> Its <span class="math inline">\((k+1)^{st}\)</span> row and column are all zero, except for the <span class="math inline">\((k+1),(k+1)\)</span> entry, which is <span class="math inline">\(N\)</span>.</p>
<p><strong>Proof:</strong> This follows from the fact that the last row and column entries are (for <span class="math inline">\(i\not=k+1\)</span>): <span class="math display">\[
(X[:,i]-\mu_{i}X[:,k+1])\cdot X[:,k+1] = (X[:,i]\cdot X[:,k+1])-N\mu_{i} = 0
\]</span> and for <span class="math inline">\(i=k+1\)</span> we have <span class="math inline">\(X[:,k+1]\cdot X[:,k+1]=N\)</span> since that column is just <span class="math inline">\(N\)</span> <span class="math inline">\(1\)</span>’s.</p>
<h3 id="exercises-1"><span class="header-section-number">1.5.3</span> Exercises</h3>
<ol type="1">
<li>When proving that <span class="math inline">\(D\)</span> is invertible if and only if the columns of <span class="math inline">\(X\)</span> are linearly independent, we argued that if <span class="math inline">\(X^{\intercal}Xm=0\)</span> for a nonzero vector <span class="math inline">\(m\)</span>, it must be the case that <span class="math inline">\(Xm\)</span> is orthogonal to every column of <span class="math inline">\(X\)</span> and so <span class="math inline">\(Xm=0\)</span>. Flesh out this argument. For example, why is it the case that if <span class="math inline">\(v\in\mathbf{R}^{N}\)</span> is a linear combination of vectors <span class="math inline">\(x_1,\ldots, x_k\)</span> in <span class="math inline">\(\mathbf{R}^{N}\)</span>, and <span class="math inline">\(v\cdot x_i=0\)</span> for <span class="math inline">\(i=1,\ldots, k\)</span>, then <span class="math inline">\(v=0\)</span>?</li>
</ol>
<h1 id="bibliography" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-irvine">
<p>[1] <span class="smallcaps">U.C. Irvine ML Repository</span>. Auto MPG Dataset.Available at <a href="http://https://archive.ics.uci.edu/ml/datasets/Auto+MPG" class="uri">http://https://archive.ics.uci.edu/ml/datasets/Auto+MPG</a>.</p>
</div>
</div>
</body>
</html>
