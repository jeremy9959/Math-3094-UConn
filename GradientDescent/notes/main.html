<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Gradient Descent</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>



<h1 id="gradient-descent"><span class="header-section-number">1</span> Gradient Descent</h1>
<h2 id="basic-algorithm"><span class="header-section-number">1.1</span> Basic Algorithm</h2>
<p>Consider a function <span class="math inline">\(E: \mathbb R^n \rightarrow \mathbb R\)</span>, <span class="math inline">\({\boldsymbol{w}}=(w_1, w_2, \dots , w_n) \rightarrow E({\boldsymbol{w}})\)</span>. The <em>gradient</em> <span class="math inline">\(\nabla E\)</span> of <span class="math inline">\(E\)</span> is defined by <span class="math display">\[  \nabla E := \left ( \frac {\partial E}{\partial w_1}, \frac {\partial E}{\partial w_2}, \dots , \frac {\partial E}{\partial w_n} \right ). \]</span></p>
<p><strong>Proposition </strong>: <em>Assume that <span class="math inline">\(E({\boldsymbol{w}})\)</span> is differentiable in a neighborhood of <span class="math inline">\({\boldsymbol{w}}\)</span>. Then the function <span class="math inline">\(E({\boldsymbol{w}})\)</span> decreases fastest in the direction of <span class="math inline">\(-\nabla E ({\boldsymbol{w}})\)</span>.</em></p>
<p>Set <span class="math display">\[ {\boldsymbol{w}}_{k+1}={\boldsymbol{w}}_k - \eta \nabla E({\boldsymbol{w}}_k) \]</span> where <span class="math inline">\(\eta &gt;0\)</span> is the step size or <em>learning rate</em>. Then <span class="math display">\[ E({\boldsymbol{w}}_k) \le E({\boldsymbol{w}}_{k+1}).\]</span> Under some moderate conditions, <span class="math display">\[ E({\boldsymbol{w}}_k) \rightarrow \text{local minimum} \qquad \text{ as }\ k \rightarrow \infty .\]</span></p>
<h3 id="example"><span class="header-section-number">1.1.1</span> Example</h3>
<p>Consider <span class="math inline">\(E({\boldsymbol{w}})=E(w_1,w_2)= w_1^4+w_2^4-16w_1w_2\)</span>. Then <span class="math inline">\(\nabla E({\boldsymbol{w}})= [ 4w_1^3-16w_2, 4w_2^3-16w_1]\)</span>. Choose <span class="math inline">\({\boldsymbol{w}}_0=(1,1)\)</span> and <span class="math inline">\(\eta =0.01\)</span>. Then <span class="math display">\[ {\boldsymbol{w}}_{30}=(1.99995558586289, 1.99995558586289)\]</span> and we get <span class="math display">\[ E({\boldsymbol{w}}_{30})= -31.9999999368777. \]</span></p>
<p>We see that <span class="math inline">\({\boldsymbol{w}}_k \rightarrow (2,2)\)</span> and <span class="math inline">\(E(2,2)=-32\)</span>. Indeed, using multi-variable calculus, one can verify that when <span class="math inline">\({\boldsymbol{w}}=(2,2)\)</span>, a local minimum of <span class="math inline">\(E({\boldsymbol{w}})\)</span> is <span class="math inline">\(-32\)</span>.</p>
<p><strong>Exercise</strong>: Find all the local minima of <span class="math inline">\(E({\boldsymbol{w}})\)</span>.</p>
<div id="fig:des" class="fignos">
<figure>
<img src="descent.png" alt="Figure 1: Graph of E({\boldsymbol{w}})" style="width:30.0%" /><figcaption><span>Figure 1:</span> Graph of <span class="math inline">\(E({\boldsymbol{w}})\)</span></figcaption>
</figure>
</div>
<div id="fig:tab" class="fignos">
<figure>
<img src="table.png" alt="Figure 2: Values of E({\boldsymbol{w}}_k)" style="width:50.0%" /><figcaption><span>Figure 2:</span> Values of <span class="math inline">\(E({\boldsymbol{w}}_k)\)</span></figcaption>
</figure>
</div>
<h2 id="newtons-method"><span class="header-section-number">1.2</span> Newton’s Method</h2>
<ul>
<li><p><span class="math inline">\(f(x)\)</span>: single-variable (convex, differentiable) function<br />
To find a local minimum <span class="math inline">\(\Longleftrightarrow\)</span> To find <span class="math inline">\(x^*\)</span> such that <span class="math inline">\(f&#39;(x^*)=0\)</span><br />
Make a guess <span class="math inline">\(x_0\)</span> for <span class="math inline">\(x^*\)</span> and set <span class="math inline">\(x=x_0+h\)</span>.</p></li>
<li><p>Using the Taylor expansion, we have <span class="math display">\[\begin{align*} f(x) &amp;=f(x_0+h) \approx f(x_0) + f&#39;(x_0) h+ \tfrac 1 2 f&#39;&#39;(x_0) h^2 \\ f&#39;(x) = \frac{df}{dh}\frac {dh}{dx} &amp; \approx \frac d {dh} \left ( f(x_0) + f&#39;(x_0) h+ \tfrac 1 2 f&#39;&#39;(x_0) h^2 \right ) \\
&amp;= f&#39;(x_0) + f&#39;&#39;(x_0)h \end{align*}\]</span> If <span class="math inline">\(0 = f&#39;(x_0) + f&#39;&#39;(x_0)h\)</span>, we obtain <span class="math display">\[ h = - f&#39;(x_0)/f&#39;&#39;(x_0). \]</span></p></li>
<li><p>We have shown that <span class="math display">\[ x_1=x_0- f&#39;(x_0)/f&#39;&#39;(x_0) \]</span> is an approximation of <span class="math inline">\(x^*\)</span>.</p></li>
<li><p>Repeat the process to obtain <span class="math display">\[ \boxed{x_{k+1}=x_k- f&#39;(x_k)/f&#39;&#39;(x_k)},  \]</span> and <span class="math inline">\(x_k \rightarrow x^*\)</span> as <span class="math inline">\(k \rightarrow \infty\)</span>.</p></li>
<li><p><span class="math inline">\(E({\boldsymbol{w}})\)</span>: multi-variable function<br />
The <em>Hessian matrix</em> of <span class="math inline">\(E\)</span> is defined by <span class="math display">\[ \mathbf H E= \begin{bmatrix} \tfrac {\partial^2 E}{\partial w_1^2} &amp;  \tfrac {\partial^2 E}{\partial w_1 \partial w_2} &amp; \cdots &amp;  \tfrac {\partial^2 E}{\partial w_1 \partial w_m} \\ \tfrac {\partial^2 E}{\partial w_2 \partial w_1} &amp; \tfrac {\partial^2 E}{\partial w_2^2} &amp; \cdots &amp; \tfrac {\partial^2 E}{\partial w_2 \partial w_m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \tfrac {\partial^2 E}{\partial w_m \partial w_1} &amp; \tfrac {\partial^2 E}{\partial w_m \partial w_2} &amp; \cdots &amp; \tfrac {\partial^2 E}{\partial w_m^2}  \end{bmatrix}. \]</span></p></li>
</ul>
<p>That is, <span class="math inline">\(\mathbf H E= [ \tfrac {\partial^2 E}{\partial w_i \partial w_j}]\)</span>.</p>
<ul>
<li><p>Generalizing the single-variable case, we obtain<br />
<span class="math display">\[ \boxed{ {\boldsymbol{w}}_{k+1}= {\boldsymbol{w}}_k - \mathbf H E ({\boldsymbol{w}}_{k})^{-1} \nabla E({\boldsymbol{w}}_k)} . \]</span></p></li>
<li><p>Using a step size <span class="math inline">\(\eta\)</span>, the formula may be modified to be <span class="math display">\[  \boxed{ {\boldsymbol{w}}_{k+1}= {\boldsymbol{w}}_k - \eta \mathbf H E ({\boldsymbol{w}}_{k})^{-1} \nabla E({\boldsymbol{w}}_k)} . \]</span></p></li>
<li><p>Newton’s method is much faster than Gradient Descent. However, it may be expensive to compute the inverse of the Hessian matrix.</p></li>
</ul>
<h3 id="example-1"><span class="header-section-number">1.2.1</span> Example</h3>
<ul>
<li>Consider <span class="math inline">\(E({\boldsymbol{w}})=E(w_1,w_2)= w_1^4+w_2^4-16w_1w_2\)</span>. Then <span class="math inline">\(\nabla E({\boldsymbol{w}})= [ 4w_1^3-16w_2, 4w_2^3-16w_1]^\top\)</span>.</li>
</ul>
<p><span class="math display">\[\begin{align*} \mathbf H E({\boldsymbol{w}}) &amp;= \begin{bmatrix} 12 w_1^2 &amp; -16 \\ -16 &amp; 12 w_2^2 \end{bmatrix} \\ \\ \mathbf H E({\boldsymbol{w}})^{-1} &amp;= \frac 1 {9w_1^2 w_2^2 -16} \begin{bmatrix} \tfrac 3 4 w_2^2 &amp; 1 \\ 1&amp; \frac 3 4 w_1^2  \end{bmatrix} \\  \\
\mathbf H E^{-1} \nabla E &amp; = \frac 1 {9w_1^2 w_2^2 -16} \begin{bmatrix} 3 w_1^3 w_2^2 -8 w_2^3 -16w_1 \\ 3 w_1^2 w_2^3 -8 w_1^3  -16w_2  \end{bmatrix} \end{align*}\]</span></p>
<!---
- Choose
$\bw_0=(1,1)$ and
$\eta =1$.
Then $\bw_{1}=(2,2)$
 and $E(\bw_{1})= -32$.
-->
<ul>
<li>Choose <span class="math inline">\({\boldsymbol{w}}_0=(1.2,1.2)\)</span> and <span class="math inline">\(\eta =1\)</span>. Then <span class="math inline">\({\boldsymbol{w}}_{9}=(2.00000004189571, 2.00000004189571)\)</span>, <span class="math inline">\(E({\boldsymbol{w}}_{9})= -31.9999999999999\)</span>.</li>
</ul>
<div id="fig:tab" class="fignos">
<figure>
<img src="table-1.png" alt="Figure 3: Values of E({\boldsymbol{w}}_k)" style="width:50.0%" /><figcaption><span>Figure 3:</span> Values of <span class="math inline">\(E({\boldsymbol{w}}_k)\)</span></figcaption>
</figure>
</div>
<h3 id="stochastic-gradient-descent-sgd"><span class="header-section-number">1.2.2</span> Stochastic Gradient Descent (SGD)</h3>
<p>Typically in Machine Learning, the function <span class="math inline">\(E({\boldsymbol{w}})\)</span> is given by a sum of the form <span class="math display">\[ E({\boldsymbol{w}}) = \frac 1 N \sum_{n=1}^N E_n({\boldsymbol{w}}), \]</span> where <span class="math inline">\(N\)</span> is the number of elements in the training set. When <span class="math inline">\(N\)</span> is large, computation of the gradient <span class="math inline">\(\nabla E\)</span> may be expensive.</p>
<p>The SGD selects a sample from the training set in each iteration step instead of using the whole batch of the training set, and use <span class="math display">\[ \frac 1 M \sum_{i=1}^M \nabla E_{n_i}({\boldsymbol{w}}_k) ,\]</span> where <span class="math inline">\(M\)</span> is the size of the sample. The SGD is commonly used in many Machine Learning algorithms.</p>
<p><br><br><br></p>
</body>
</html>
