<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Principal Components</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
  <!-- pandoc-eqnos: equation style -->
  <style>
    .eqnos { display: inline-block; position: relative; width: 100%; }
    .eqnos br { display: none; }
    .eqnos-number { position: absolute; right: 0em; top: 50%; line-height: 0; }
  </style>
</head>
<body>

<h1 id="principal-component-analysis"><span class="header-section-number">1</span> Principal Component Analysis</h1>
<h2 id="introduction"><span class="header-section-number">1.1</span> Introduction</h2>
<p>Suppose that, as usual, we begin with a collection of measurements of different features for a group of samples. Some of these measurements will tell us quite a bit about the difference among our samples, while others may contain relatively little information. For example, if we are analyzing the effect of a certain weight loss regimen on a group of people, the age and weight of the subjects may have a great deal of influence on how successful the regimen is, while their blood pressure might not. One way to help identify which features are more significant is to ask whether or not the feature varies a lot among the different samples. If nearly all the measurements of a feature are the same, it can’t have much power in distinguishing the samples, while if the measurements vary a great deal then that feature has a chance to contain useful information.</p>
<p>In this section we will discuss a way to measure the variability of measurements and then introduce principal component analysis (PCA). PCA is a method for finding which linear combinations of measurements have the greatest variability and therefore might contain the most information. It also allows us to identify combinations of measurements that don’t vary much at all. Combining this information, we can sometimes replace our original system of features with a smaller set that still captures most of the interesting information in our data, and thereby find hidden characteristics of the data and simplify our analysis a great deal.</p>
<h2 id="variance-and-covariance"><span class="header-section-number">1.2</span> Variance and Covariance</h2>
<h3 id="variance"><span class="header-section-number">1.2.1</span> Variance</h3>
<p>Suppose that we have a collection of measurements <span class="math inline">\((x_1,\ldots, x_n)\)</span> of a particular feature <span class="math inline">\(X\)</span>. For example, <span class="math inline">\(x_i\)</span> might be the initial weight of the <span class="math inline">\(ith\)</span> participant in our weight loss study. The mean of the values <span class="math inline">\((x_1,\ldots, x_n)\)</span> is</p>
<p><span class="math display">\[
\mu_{X} = \frac{1}{n}\sum_{i=1}^{n} x_{i}.
\]</span></p>
<p>The simplest measure of the variability of the data is called its <em>variance.</em></p>
<p><strong>Definition:</strong> The (sample) variance of the data <span class="math inline">\(x_1,\ldots, x_n\)</span> is</p>
<p><span id="eq:variance" class="eqnos"><span class="math display">\[
\sigma_{X}^2 = \frac{1}{n}\sum_{i=1}^{n} (x_{i}-\mu_{X})^2 = \frac{1}{n}\sum_{i=1}^{n} x_{i}^2 - \mu_{X}^2
\]</span><span class="eqnos-number">(1)</span></span></p>
<p>The square root of the variance is called the <em>standard deviation.</em></p>
<p>As we see from the formula, the variance is a measure of how ‘spread out’ the data is from the mean.</p>
<p>Recall that in our discussion of linear regression we thought of our set of measurements <span class="math inline">\(x_1,\ldots, x_n\)</span> as a vector – it’s one of the columns of our data matrix. From that point of view, the variance has a geometric interpretation – it is <span class="math inline">\(\frac{1}{N}\)</span> times the square of the distance from the point <span class="math inline">\(X=(x_1,\ldots, x_n)\)</span> to the point <span class="math inline">\(\mu_{X}(1,1,\ldots,1)=\mu_{X}E\)</span>:</p>
<p><span id="eq:variancedot" class="eqnos"><span class="math display">\[
\sigma_{X}^2 = \frac{1}{n}(X-\mu_{X}E)\cdot(X-\mu_{X}E)  = \frac{1}{n}\|X-\mu_{X}E\|^2.
\]</span><span class="eqnos-number">(2)</span></span></p>
<h3 id="covariance"><span class="header-section-number">1.2.2</span> Covariance</h3>
<p>The variance measures the dispersion of measures of a single feature. Often, we have measurements of multiple features and we might want to know something about how two features are related. The <em>covariance</em> is a measure of whether two features tend to be related, in the sense that when one increases, the other one increases; or when one increases, the other one decreases.</p>
<p><strong>Definition:</strong> Given measurements <span class="math inline">\((x_1,\ldots, x_n)\)</span> and <span class="math inline">\((y_1,\ldots, y_n)\)</span> of two features <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is</p>
<p><span id="eq:covariancedot" class="eqnos"><span class="math display">\[
\sigma_{XY} = \frac{1}{N}\sum_{i=1}^{N} x_iy_i
\]</span><span class="eqnos-number">(3)</span></span></p>
<p>There is a nice geometric interpretation of this, as well, in terms of the dot product. If <span class="math inline">\(X=(x_1,\ldots, x_n)\)</span> and <span class="math inline">\(Y=(y_1\ldots,y_n)\)</span> then</p>
<p><span class="math display">\[
\sigma_{XY} = \frac{1}{N} ((X-\mu_{X})\cdot (Y-\mu_{Y})).
\]</span></p>
<p>From this point of view, we can see that <span class="math inline">\(\sigma_{XY}\)</span> is positive if the <span class="math inline">\(X-\mu_{X}\)</span> and <span class="math inline">\(Y-\mu_{Y}\)</span> vectors “point roughly in the same direction” and its negative if they “point roughly in the opposite direction.”</p>
<h3 id="correlation"><span class="header-section-number">1.2.3</span> Correlation</h3>
<p>One problem with interpreting the variance and covariance is that we don’t have a scale – for example, if <span class="math inline">\(\sigma_{XY}\)</span> is large and positive, then we’d like to say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are closely related, but it could be just that the entries of <span class="math inline">\(X-\mu_{X}\)</span> and <span class="math inline">\(Y-\mu_{Y}\)</span> are large. Here, though, we can really take advantage of the geometric interpretation. Recall that the dot product of two vectors satisfies the formula</p>
<p><span class="math display">\[
a \cdot b = \|a\|\|b\|\cos(\theta)
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. So</p>
<p><span class="math display">\[
\cos(\theta) = \frac{a\cdot b}{\|a\|\|b\|}.
\]</span></p>
<p>Let’s apply this to the variance and covariance, by noticing that</p>
<p><span class="math display">\[
\frac{(X-\mu_{X})\cdot (Y-\mu_{Y})}{\|(X-\mu_{X})\|\|(Y-\mu_{Y})\|} = \frac{\sigma_{XY}}{\sigma_{XX}\sigma_{YY}}
\]</span></p>
<p>so the quantity</p>
<p><span id="eq:rxy" class="eqnos"><span class="math display">\[
r_{XY} = \frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}}
\]</span><span class="eqnos-number">(4)</span></span></p>
<p>measures the cosine of the angle between the vectors <span class="math inline">\(X-\mu_{X}\)</span>A and <span class="math inline">\(Y-\mu_{Y}\)</span>.</p>
<p><strong>Definition:</strong> The quantity <span class="math inline">\(r_{XY}\)</span> defined in eq. <a href="#eq:rxy">4</a> is called the (sample) <em>correlation coefficient</em> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We have <span class="math inline">\(0\le |r_{XY}|\le 1\)</span> with <span class="math inline">\(r_{XY}=\pm 1\)</span> if and only if the two vectors <span class="math inline">\(X-\mu_{X}\)</span> and <span class="math inline">\(Y-\mu_{Y}\)</span> are collinear in <span class="math inline">\(\mathbf{R}^{n}\)</span>.</p>
<p>Figure <a href="#fig:corrfig">1</a> illustrates data with different values of the correlation coefficient.</p>
<div id="fig:corrfig" class="fignos">
<figure>
<img src="../img/correlation.png" alt="Figure 1: Correlation" style="width:50.0%" /><figcaption><span>Figure 1:</span> Correlation</figcaption>
</figure>
</div>
<h3 id="sec:covarmat"><span class="header-section-number">1.2.4</span> The covariance matrix</h3>
<p>In a typical situation we have many features for each of our (many) samples, that we organize into a data matrix <span class="math inline">\(X\)</span>. To recall, each column of <span class="math inline">\(X\)</span> corresponds to a feature that we measure, and each row corresponds to a sample. For example, each row of our matrix might correspond to a person enrolled in a study, and the columns correspond to height (cm), weight (kg), systolic blood pressure, and age (in years):</p>
<div id="tbl:data" class="tablenos">
<table>
<caption><span>Table 1:</span> A sample data matrix <span class="math inline">\(X\)</span> </caption>
<thead>
<tr class="header">
<th>sample</th>
<th style="text-align: right;">Ht</th>
<th style="text-align: right;">Wgt</th>
<th style="text-align: right;">Bp</th>
<th style="text-align: right;">Age</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td style="text-align: right;">180</td>
<td style="text-align: right;">75</td>
<td style="text-align: right;">110</td>
<td style="text-align: right;">35</td>
</tr>
<tr class="even">
<td>B</td>
<td style="text-align: right;">193</td>
<td style="text-align: right;">80</td>
<td style="text-align: right;">130</td>
<td style="text-align: right;">40</td>
</tr>
<tr class="odd">
<td>…</td>
<td style="text-align: right;">…</td>
<td style="text-align: right;">…</td>
<td style="text-align: right;">…</td>
<td style="text-align: right;">…</td>
</tr>
<tr class="even">
<td>U</td>
<td style="text-align: right;">150</td>
<td style="text-align: right;">92</td>
<td style="text-align: right;">105</td>
<td style="text-align: right;">55</td>
</tr>
</tbody>
</table>
</div>
<p>If we have multiple features, as in this example, we might be interested in the variance of each feature and all of their mutual covariances. This “package” of information can be obtained “all at once” by taking advantage of some matrix algebra.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(X\)</span> be a <span class="math inline">\(k\times N\)</span> data matrix, where the <span class="math inline">\(N\)</span> columns of <span class="math inline">\(X\)</span> correspond to different features and the <span class="math inline">\(k\)</span> rows to different samples. Let <span class="math inline">\(X_{0}\)</span> be the centered version of this data matrix, obtained by subtracting the mean <span class="math inline">\(\mu_{i}\)</span> of column <span class="math inline">\(i\)</span> from all the entries <span class="math inline">\(x_{si}\)</span> in that column. Then the <span class="math inline">\(N\times N\)</span> symmetric matrix</p>
<p><span class="math display">\[
D_{0} = \frac{1}{N}X_{0}^{\intercal}X_{0}
\]</span></p>
<p>is called the (sample) covariance matrix for the data.</p>
<p><strong>Proposition:</strong> The diagonal entries <span class="math inline">\(d_{ii}\)</span> of <span class="math inline">\(D_{0}\)</span> are the variances of the columns of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
d_{ii} = \sigma_{i}^2 = \frac{1}{N}\sum_{s=1}^{k}(x_{si}-\mu_i)^2
\]</span></p>
<p>and the off-diagonal entries <span class="math inline">\(d_{ij} = d_{ji}\)</span> are the covariances of the <span class="math inline">\(i^{th}\)</span> and <span class="math inline">\(j^{th}\)</span> columns of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
d_{ij} = \sigma_{ij} = \frac{1}{N}\sum_{s=1}^{k}(x_{si}-\mu_{i})(x_{sj}-\mu_{j})
\]</span></p>
<p><strong>Proof:</strong> This follows from the definitions, but it’s worth checking the details, which we leave as an exercise.</p>
<h3 id="linear-combinations-of-features-scores"><span class="header-section-number">1.2.5</span> Linear Combinations of Features (Scores)</h3>
<p>Sometimes useful information about our data can be revealed if we combine different measurements together to obtain a “hybrid” measure that captures something interesting. For example, in the Auto MPG dataset that we studied in the section on Linear Regression, we looked at the influence of both vehicle weight <span class="math inline">\(w\)</span> and engine displacement <span class="math inline">\(e\)</span> on gas mileage; perhaps their is some value in considering a hybrid “score” defined as <span class="math display">\[
S = a*w + b*e
\]</span> for some constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> – maybe by choosing a good combination we could find a better predictor of gas mileage than using one or the other of the features individually.</p>
<p>As another example, suppose we are interested in the impact of the nutritional content of food on weight gain in a study. We know that both calorie content and the level dietary fiber contribute to the weight gain of participants eating this particular food; maybe there is some kind of combined “calorie/fiber” score we could introduce that captures the impact of that food better.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(X_{0}\)</span> be a (centered) <span class="math inline">\(k\times N\)</span> data matrix giving information about <span class="math inline">\(N\)</span> features for each of <span class="math inline">\(k\)</span> samples. A linear synthetic feature, or a linear score, is a linear combination of the <span class="math inline">\(N\)</span> features. The linear score is defined by constants <span class="math inline">\(a_{1},\ldots, a_{n}\)</span> so that If <span class="math inline">\(y_{1},\ldots, y_{N}\)</span> are the values of the features for a particular sample, then the linear score for that sample is</p>
<p><span class="math display">\[
S = a_{1}y_{1}+a_{2}y_{2}+\cdots+a_{N}y_{N}
\]</span></p>
<p><strong>Lemma:</strong> The values of the linear score for each of the <span class="math inline">\(k\)</span> samples can be calculated as</p>
<p><span id="eq:linearscore" class="eqnos"><span class="math display">\[
\left[\begin{matrix} S_{1} \\ \vdots \\ S_{k}\\ \end{matrix}\right] =
X_{0}\left[
\begin{matrix} a_{1} \\ \vdots \\ a_{N}\end{matrix}\right].
\]</span><span class="eqnos-number">(5)</span></span></p>
<p><strong>Proof:</strong> Multiplying a matrix by a column vector computes a linear combination of the columns – that’s what this lemma says. Exercise 3 asks you to write out the indices and make sure you believe this.</p>
<h3 id="mean-and-variance-of-scores"><span class="header-section-number">1.2.6</span> Mean and variance of scores</h3>
<p>When we combine features to make a hybrid score, we assume that the features were centered to begin with, so that each features has mean zero. As a result, the mean of the hybrid features is again zero.</p>
<p><strong>Lemma:</strong> A linear combination of features with mean zero again has mean zero.</p>
<p><strong>Proof:</strong> Let <span class="math inline">\(S_{i}\)</span> be the score for the <span class="math inline">\(i^{th}\)</span> sample, so <span class="math display">\[
S_{i} = \sum_{j=1}^{N} x_{ij}a_{j}.
\]</span> where <span class="math inline">\(X_{0}\)</span> has entries <span class="math inline">\(x_{ij}\)</span>. Then the mean value of the score is <span class="math display">\[
\mu_{S} = \frac{1}{k}\sum_{i=1}^{k} S_{i} = \frac{1}{k}\sum_{i=1}^{k}\sum_{j=1}^{N} x_{ij}a_{j}.
\]</span> Reversing the order of the sum yields <span class="math display">\[
\mu_{S} = \frac{1}{k}\sum_{j=1}^{N}\sum_{i=1}^{k} x_{ij}a_{j} = \sum_{j=1}^{N} a_{j}\frac{1}{k}(\sum_{i=1}^{k} x_{ij})=
\sum_{j=1}^{N}a_{j}\mu_{j}=0
\]</span> where <span class="math inline">\(\mu_{j}=0\)</span> is the mean of the <span class="math inline">\(j^{th}\)</span> feature (column) of <span class="math inline">\(X_{0}\)</span>.</p>
<p>The variance is more interesting, and gives us an opportunity to put the covariance matrix to work. Remember from <a href="#eq:variancedot">2</a> that, since a score <span class="math inline">\(S\)</span> has mean zero, it’s variance is <span class="math inline">\(\sigma_{S}^2=S\cdot S\)</span> – where here the score <span class="math inline">\(S\)</span> is represented by the column vector with entries <span class="math inline">\(S_{1},\ldots S_{k}\)</span> as in eq. <a href="#eq:linearscore">5</a>.</p>
<p><strong>Lemma:</strong> The variance of the score <span class="math inline">\(S\)</span> with weights <span class="math inline">\(a_1,\ldots a_N\)</span> is <span id="eq:ada" class="eqnos"><span class="math display">\[
\sigma_{S}^2 = a^{\intercal}D_{0}a = \left[\begin{matrix}a_{1} &amp; \cdots &amp; a_{N}\end{matrix}\right]D_{0}
\left[\begin{matrix} a_{1} \\ \vdots \\ a_{N}\end{matrix}\right]
\]</span><span class="eqnos-number">(6)</span></span> More generally, if <span class="math inline">\(S_{1}\)</span> and <span class="math inline">\(S_{2}\)</span> are scores with weights <span class="math inline">\(a_1,\ldots, a_N\)</span> and <span class="math inline">\(b_1,\ldots, b_N\)</span> respectively, then the covariance <span class="math inline">\(\sigma_{S_{1}S_{2}}\)</span> is <span class="math display">\[
\sigma_{S_{1}S_{2}} = a^{\intercal}D_{0}b.
\]</span></p>
<p><strong>Proof:</strong> From eq. <a href="#eq:variancedot">2</a> and <a href="#eq:linearscore">5</a> we know that <span class="math display">\[
\sigma_{S}^2 = S\cdot S
\]</span> and <span class="math display">\[
S = X_{0}a.
\]</span> Since <span class="math inline">\(S\cdot S = \frac{1}{N}S^{\intercal}S\)</span>, this gives us <span class="math display">\[
\sigma_{S}^2 = \frac{1}{N}(X_{0}a)^{\intercal}(X_{0}a) = \frac{1}{N}a^{\intercal}X_{0}^{\intercal}X_{0}a = a^{\intercal}D_{0}a
\]</span> as claimed.</p>
<p>For the covariance, use a similar argument with eq. <a href="#eq:covariancedot">3</a> and eq. <a href="#eq:linearscore">5</a>. writing <span class="math inline">\(\sigma_{S_{1}S_{2}}=\frac{1}{N}S_{1}\cdot S_{2}\)</span> and the fact that <span class="math inline">\(S_{1}\)</span> and <span class="math inline">\(S_{2}\)</span> can be written as <span class="math inline">\(X_{0}a\)</span> and <span class="math inline">\(X_{0}b\)</span>.</p>
<p>The point of this lemma is that the covariance matrix contains not just the variances and covariances of the original features, but also enough information to construct the variances and covariances for <em>any linear combination of features.</em></p>
<p>In the next section we will see how to exploit this idea to reveal hidden structure in our data.</p>
<h3 id="geometry-of-scores"><span class="header-section-number">1.2.7</span> Geometry of Scores</h3>
<p>Let’s begin by looking at fig. <a href="#fig:pcasimfig">2</a>, which shows a scatter plot of some simulated data having <span class="math inline">\(50\)</span> samples and two features. This data has been centered, so it can be represented in a <span class="math inline">\(50\times 2\)</span> data matrix <span class="math inline">\(X_{0}\)</span> each row of which is the coordinates <span class="math inline">\((x_0,x_1)\)</span> of one of the points in the picture.</p>
<p>The scatter plot shows that the data points are arranged in a more or less elliptical cloud oriented at an angle to the <span class="math inline">\(xy\)</span>-axes which represent the two given features. The two individual histograms show the distribution of the two features – each has mean zero, with the <span class="math inline">\(x\)</span>-features distributed between <span class="math inline">\(-2\)</span> and <span class="math inline">\(2\)</span> and the <span class="math inline">\(y\)</span> feature between <span class="math inline">\(-4\)</span> and <span class="math inline">\(4\)</span>. Looking just at the two features individually, meaning only at the two histograms, we can’t see the overall elliptical structure.</p>
<div id="fig:pcasimfig" class="fignos">
<figure>
<img src="../img/PCAsimulated-1.png" alt="Figure 2: Simulated Data with Two Features" style="width:50.0%" /><figcaption><span>Figure 2:</span> Simulated Data with Two Features</figcaption>
</figure>
</div>
<p>How can we get a better grip on our data in this situation? We can try to find a “direction” in our data that better illuminates the variation of the data. For example, suppose that we pick a unit vector at the origin pointing in a particular direction in our data. See fig. <a href="#fig:pcasimfig-1">3</a>.</p>
<div id="fig:pcasimfig-1" class="fignos">
<figure>
<img src="../img/PCAsimulated-2.png" alt="Figure 3: A direction in the data" style="width:50.0%" /><figcaption><span>Figure 3:</span> A direction in the data</figcaption>
</figure>
</div>
<p>Now we can orthogonally project the datapoints onto the line defined by this vector, as shown in fig. <a href="#fig:pcasimfig-2">4</a>.</p>
<div id="fig:pcasimfig-2" class="fignos">
<figure>
<img src="../img/PCAsimulated-3.png" alt="Figure 4: Projecting the datapoints" style="width:50.0%" /><figcaption><span>Figure 4:</span> Projecting the datapoints</figcaption>
</figure>
</div>
<p>Recall that if the unit vector is defined by coordinates <span class="math inline">\(u=[u_0,u_1]\)</span>, then the orthogonal projection of the point <span class="math inline">\(x\)</span> with coordinates <span class="math inline">\((x_0,x_1)\)</span> is <span class="math inline">\((x\cdot u)u\)</span>. Now <span class="math display">\[
x\cdot u = u_0 x_0 + u_1 x_1
\]</span> so the coordinates of the points along the line defined by <span class="math inline">\(u\)</span> are the values of the score <span class="math inline">\(Z\)</span> defined by <span class="math inline">\(u=[u_0,u_1]\)</span>. Using our work in the previous section, we see that we can find all of these coordinates by matrix multiplication: <span class="math display">\[
Z = X_0 u
\]</span> where <span class="math inline">\(X_0\)</span> is our data matrix. Now let’s add a histogram of the values of <span class="math inline">\(Z\)</span> to our picture:</p>
<div id="fig:pcasimfig-3" class="fignos">
<figure>
<img src="../img/PCAsimulated-4.png" alt="Figure 5: Distribution of Z" style="width:50.0%" /><figcaption><span>Figure 5:</span> Distribution of Z</figcaption>
</figure>
</div>
<p>This histogram shows the distribution of the values of <span class="math inline">\(Z\)</span> along the tilted line defined by the unit vector <span class="math inline">\(u\)</span>.</p>
<p>Finally, using our work on the covariance matrix, we see that the variance of <span class="math inline">\(Z\)</span> is given by <span class="math display">\[
\sigma_{Z}^2 = \frac{1}{50}u^{\intercal}X_{0}^{\intercal}X_{0}u = u^{\intercal}D_{0}u
\]</span> where <span class="math inline">\(D_{0}\)</span> is the covariance matrix of the data <span class="math inline">\(X_{0}\)</span>.</p>
<p><strong>Lemma:</strong> Let <span class="math inline">\(X_{0}\)</span> be a <span class="math inline">\(k\times N\)</span> centered data matrix, and let <span class="math inline">\(D_{0}=\frac{1}{N}X_{0}^{\intercal}X_{0}\)</span> be the associated covariance matrix. Let <span class="math inline">\(u\)</span> be a unit vector in “feature space” <span class="math inline">\(\mathbf{R}^{N}\)</span>. Then the score <span class="math inline">\(S=X_{0}u\)</span> can be interpreted as the coordinates of the points of <span class="math inline">\(X_{0}\)</span> projected onto the line generated by <span class="math inline">\(u\)</span>. The variance <span class="math display">\[
\sigma^{2}_{S} = u^{\intercal}D_{0}u = \sum_{i=1}^{k} s_{i}^2
\]</span> where <span class="math inline">\(s_{i} = X_{0}[i,:]u\)</span> is the dot product of the <span class="math inline">\(i^{th}\)</span> row <span class="math inline">\(X_{0}[i,:]\)</span> with <span class="math inline">\(u\)</span>. It measures the variability in the data “in the direction of the unit vector <span class="math inline">\(u\)</span>”.</p>
<h3 id="principal-components"><span class="header-section-number">1.2.8</span> Principal Components</h3>
<h4 id="change-of-variance-with-direction"><span class="header-section-number">1.2.8.1</span> Change of variance with direction</h4>
<p>As we’ve seen in the previous section, if we choose a unit vector <span class="math inline">\(u\)</span> in the feature space and find the projection <span class="math inline">\(X_{0}u\)</span> of our data onto the line through <span class="math inline">\(u\)</span>, we get a “score” that we can use to measure the variance of the data in the direction of <span class="math inline">\(u\)</span>. What happens as we vary <span class="math inline">\(u\)</span>?</p>
<p>To study this question, let’s continue with our simulated data from the previous section, and introduce a unit vector <span class="math display">\[
u(\theta) = \left[\begin{matrix} \cos(\theta) &amp; \sin(\theta)\end{matrix}\right].
\]</span> This is in fact a unit vector, since <span class="math inline">\(\sin^2(\theta)+\cos^2(\theta)=1\)</span>, and it is oriented at an angle <span class="math inline">\(\theta\)</span> from the <span class="math inline">\(x\)</span>-axis.</p>
<p>The variance of the data in the direction of <span class="math inline">\(u(\theta)\)</span> is given by <span class="math display">\[
\sigma_{\theta}^2 = u(\theta)^{\intercal}D_{0}u(\theta).
\]</span></p>
<p>A plot of this function for the data we have been considering is in fig. <a href="#fig:pcatheta">6</a>. As you can see, the variance goes through two full periods with the angle, and it reaches a maximum and minimum value at intervals of <span class="math inline">\(\pi/2\)</span> – so the two angles where the variance are maximum and minimum are orthogonal to one another.</p>
<div id="fig:pcatheta" class="fignos">
<figure>
<img src="../img/PCAtheta.png" alt="Figure 6: Change of variance with angle theta" style="width:25.0%" /><figcaption><span>Figure 6:</span> Change of variance with angle theta</figcaption>
</figure>
</div>
<p>The two directions where the variance is maximum and minimum are drawn on the original data scatter plot in fig. <a href="#fig:pcaprincipal">7</a> .</p>
<div id="fig:pcaprincipal" class="fignos">
<figure>
<img src="../img/PCAprincipal.png" alt="Figure 7: Data with principal directions" style="width:25.0%" /><figcaption><span>Figure 7:</span> Data with principal directions</figcaption>
</figure>
</div>
<p>Let’s try to understand why this is happening.</p>
<h4 id="directions-of-extremal-variance"><span class="header-section-number">1.2.8.2</span> Directions of extremal variance</h4>
<p>Given our centered, <span class="math inline">\(k\times N\)</span> data matrix <span class="math inline">\(X_{0}\)</span>, with its associated covariance matrix <span class="math inline">\(D_{0}=\frac{1}{N}X_{0}^{\intercal}X_{0}\)</span>, we would like to find unit vectors <span class="math inline">\(u\)</span> in <span class="math inline">\(\mathbf{R}^{N}\)</span> so that <span class="math display">\[
\sigma_{u}^{2} = u^{\intercal}D_{0}u
\]</span> reaches its maximum and its minimum. Here <span class="math inline">\(\sigma_{u}^2\)</span> is the variance of the “linear score” <span class="math inline">\(X_{0}u\)</span> and it represents how dispersed the data is in the “u direction” in <span class="math inline">\(\mathbf{R}^{N}\)</span>.</p>
<p>In this problem, remember that the coordinates of <span class="math inline">\(u=(u_1,\ldots, u_{N})\)</span> are the variables and the symmetric matrix <span class="math inline">\(D_{0}\)</span> is given. As usual, we to find the maximum and minimum values of <span class="math inline">\(\sigma_{u}^{2}\)</span>, we should look at the partial derivatives of <span class="math inline">\(\sigma_{u}^{2}\)</span> with respect to the variables <span class="math inline">\(u_{i}\)</span> and set them to zero. Here, however, there is a catch – we want to restrict <span class="math inline">\(u\)</span> to being a unit vector, with <span class="math inline">\(u\cdot u =\sum u_{i}^2=1\)</span>.</p>
<p>So this is a <em>constrained optimization problem</em>:</p>
<ul>
<li>Find extreme values of the function <span class="math display">\[
\sigma_{u}^{2} = u^{\intercal}D_{0}u
\]</span></li>
<li>Subject to the constraint <span class="math inline">\(\|u\|^2 = u\cdot u=1\)</span> (or <span class="math inline">\(u\cdot u-1=0\)</span>)</li>
</ul>
<p>As we learned in multivariate calculus, we can use the technique of <em>Lagrange Multipliers</em> to solve such a problem.</p>
<p>To apply this method, we introduce the function</p>
<p><span id="eq:lagrange" class="eqnos"><span class="math display">\[
S(u, \lambda) = u^{\intercal}D_{0}u - \lambda(u\cdot u -1)
\]</span><span class="eqnos-number">(7)</span></span></p>
<p>Then we compute the gradient</p>
<p><span id="eq:lagrangegradient" class="eqnos"><span class="math display">\[
\nabla S = \left[\begin{matrix} \frac{\partial S}{\partial u_{1}} \\ \vdots \\ \frac{\partial S}{\partial u_{N}} \\ \frac{\partial S}{\partial \lambda}\end{matrix}\right]
\]</span><span class="eqnos-number">(8)</span></span></p>
<p>and solve the system of equations <span class="math inline">\(\nabla S=0\)</span>. Here we have written the gradient as a column vector for reasons that will become clearer shortly.</p>
<p>Computing all of these partial derivatives looks messy, but actually if we take advantage of matrix algebra it’s not too bad. The following two lemmas explain how to do this.</p>
<p><strong>Lemma</strong>: Let <span class="math inline">\(M\)</span> be a <span class="math inline">\(k\times N\)</span> matrix with constant coefficients and let <span class="math inline">\(u\)</span> be a <span class="math inline">\(N\times 1\)</span> column vector whose entries are <span class="math inline">\(u_1,\ldots u_{N}\)</span>. The function <span class="math inline">\(F(u) = Mu\)</span> is a linear map from <span class="math inline">\(\mathbf{R}^{N}\to\mathbf{R}^{k}\)</span>. Its (total) derivative is a linear map between the same vector spaces, and satisfies <span class="math display">\[
D(F)(v) = Mv
\]</span> for any <span class="math inline">\(N\times 1\)</span> vector <span class="math inline">\(v\)</span>. If <span class="math inline">\(u\)</span> is a <span class="math inline">\(1\times k\)</span> matrix, and <span class="math inline">\(G(u) = uM\)</span>, then <span class="math display">\[
D(G)(v) = vM
\]</span> for any <span class="math inline">\(1\times k\)</span> vector <span class="math inline">\(v\)</span>. (This is the matrix version of the derivative rule that <span class="math inline">\(\frac{d}{dx}(ax)=a\)</span> for a constant <span class="math inline">\(a\)</span>.)</p>
<p><strong>Proof:</strong> Since <span class="math inline">\(F:\mathbf{R}^{N}\to\mathbf{R}^{k}\)</span>, we can write out <span class="math inline">\(F\)</span> in more traditional function notation as <span class="math display">\[
F(u) = (F_{1}(u_1,\ldots, u_N), \ldots, F_{k}(u_1,\ldots, u_{N})
\]</span> where <span class="math display">\[
F_{i}(u_1,\ldots u_N) = \sum_{j=1}^{N} m_{ij}u_{j}.
\]</span> Thus <span class="math inline">\(\frac{\partial F_{i}}{\partial u_{j}} = m_{ij}\)</span>. The total derivative <span class="math inline">\(D(F)\)</span> is the linear map with matrix <span class="math display">\[
D(F)_{ij} = \frac{\partial F_{i}}{\partial u_{j}} = m_{ij} = M.
\]</span> The other result is proved the same way.</p>
<p><strong>Lemma</strong>: Let <span class="math inline">\(D\)</span> be a symmetric <span class="math inline">\(N\times N\)</span> matrix with constant entries and let <span class="math inline">\(u\)</span> be an <span class="math inline">\(N\times 1\)</span> column vector of variables <span class="math inline">\(u_{1},\ldots, u_{N}\)</span>. Let <span class="math inline">\(F:\mathbf{R}^{N}\to R\)</span> be the function <span class="math inline">\(F(u) = u^{\intercal}Du\)</span>. Then the derivative gradient <span class="math inline">\(\nabla_{u} F\)</span> is a vector field – that is, a vector-valued function of <span class="math inline">\(u\)</span>, and is given by the formula <span class="math display">\[
\nabla_{u} F = 2Du
\]</span></p>
<p><strong>Proof:</strong> Let <span class="math inline">\(d_{ij}\)</span> be the <span class="math inline">\(i,j\)</span> entry of <span class="math inline">\(D\)</span>. We can write out the function <span class="math inline">\(F\)</span> to obtain <span class="math display">\[
F(u_1,\ldots, u_{N}) = \sum_{i=1}^{N} \sum_{j=1}^{N} u_i d_{ij} u_j.
\]</span> Now <span class="math inline">\(\frac{\partial F}{\partial u_{i}}\)</span> is going to pick out only terms where <span class="math inline">\(u_{i}\)</span> appears, yielding: <span class="math display">\[
\frac{\partial F}{\partial u_{i}} = \sum_{j=1}^{N} d_{ij}u_{j} + \sum_{j=1}^{N} u_{j}d_{ji}
\]</span> Here the first sum catches all of the terms where the first “u” is <span class="math inline">\(u_{i}\)</span>; and the second sum catches all the terms where the second “u” is <span class="math inline">\(u_{i}\)</span>. The diagonal terms <span class="math inline">\(u_{i}^2d_{ii}\)</span> contribute once to each sum, which is consistent with the rule that the derivative of <span class="math inline">\(u_{i}^2d_{ii} = 2u_{i}d_{ii}\)</span>. To finish the proof, notice that <span class="math display">\[
\sum_{j=1}^{N} u_{j}d_{ji} = \sum_{j=1}^{N} d_{ij}u_{j} 
\]</span> since <span class="math inline">\(D\)</span> is symmetric, so in fact the two terms are the same Thus <span class="math display">\[
\frac{\partial}{\partial u_{i}}F = 2\sum_{j=1}^{N} d_{ij}u_{j}
\]</span> But the right hand side of this equation is twice the <span class="math inline">\(i^{th}\)</span> of <span class="math inline">\(Du\)</span>, so putting the results together we get <span class="math display">\[
\nabla_{u}F = \left[\begin{matrix} \frac{\partial F}{\partial u_{1}} \\ \vdots \\ \frac{\partial F}{\partial u_{N}}\end{matrix}\right] = 2Du.
\]</span></p>
<p>The following theorem puts all of this work together to answer our questions about how variance changes with direction.</p>
<p><strong>Theorem:</strong> The critical values of the variance <span class="math inline">\(\sigma_{u}^2\)</span>, as <span class="math inline">\(u\)</span> varies over unit vectors in <span class="math inline">\(\mathbf{R}^{N}\)</span>, are the eigenvalues <span class="math inline">\(\lambda_{1},\ldots,\lambda_{N}\)</span> of the covariance matrix <span class="math inline">\(D\)</span>, and if <span class="math inline">\(e_{i}\)</span> is a unit eigenvector corresponding to <span class="math inline">\(\lambda_{i}\)</span>, then <span class="math inline">\(\sigma_{e_{i}}^2 = \lambda_{i}\)</span>.</p>
<p><strong>Proof:</strong> Recall that we introduced the Lagrange function <span class="math inline">\(S(u,\lambda)\)</span>, whose critical points give us the solutions to our constrained optimization problem. As we said in eq. <a href="#eq:lagrange">7</a>: <span class="math display">\[
S(u,\lambda) = u^{\intercal}D_{0}u - \lambda(u\cdot u - 1) = u^{\intercal}D_{0}u -\lambda(u\cdot u) + \lambda
\]</span> Now apply our Matrix calculus lemmas. First, let’s treat <span class="math inline">\(\lambda\)</span> as a constant and focus on the <span class="math inline">\(u\)</span> variables. We can write <span class="math inline">\(u\cdot u = u^{\intercal} I_{N} u\)</span> where <span class="math inline">\(I_{N}\)</span> is the identity matrix to compute: <span class="math display">\[
\nabla_{u} S = 2D_{0}u -2\lambda u
\]</span> For <span class="math inline">\(\lambda\)</span> we have <span class="math display">\[
\frac{\partial}{\partial \lambda}S = -u\cdot u +1.
\]</span> The critical points occur when <span class="math display">\[
\nabla_{u} S = 2(D_{0}-\lambda)u = 0
\]</span> and <span class="math display">\[
\frac{\partial}{\partial \lambda}S = 1-u\cdot u = 0
\]</span> The first equation says that <span class="math inline">\(\lambda\)</span> must be an eigenvalue, and <span class="math inline">\(u\)</span> and eigenvector: <span class="math display">\[
D_{0}u = \lambda u
\]</span> while the second says <span class="math inline">\(u\)</span> must be a unit vector <span class="math inline">\(u\cdot u=\|u\|^2=1\)</span>.</p>
<p><strong>Exercises.</strong></p>
<ol type="1">
<li><p>Prove that the two expressions for <span class="math inline">\(\sigma_{X}^2\)</span> given in section <a href="#variance">1.2.1</a> are the same.</p></li>
<li><p>Prove that the covariance matrix is as described in the proposition in <a href="#sec:covarmat">1.2.4</a>.</p></li>
<li><p>Let <span class="math inline">\(X_{0}\)</span> be a <span class="math inline">\(k\times N\)</span> matrix with entries <span class="math inline">\(x_{ij}\)</span> for <span class="math inline">\(1\le i\le k\)</span> and <span class="math inline">\(1\le j\le N\)</span>. If a linear score is defined by the constants <span class="math inline">\(a_{1},\ldots a_{N}\)</span>, check that equation eq. <a href="#eq:linearscore">5</a> holds as claimed.</p></li>
<li><p>Why is it important to use a unit vector when computing the variance of <span class="math inline">\(X_{0}\)</span> in the direction of <span class="math inline">\(u\)</span>? Suppose <span class="math inline">\(v=\lambda u\)</span> where <span class="math inline">\(u\)</span> is a unit vector and <span class="math inline">\(\lambda&gt;0\)</span> is a constant. Let <span class="math inline">\(S&#39;\)</span> be the score <span class="math inline">\(X_{0}v\)</span>. How is the variance of <span class="math inline">\(S&#39;\)</span> related to that of <span class="math inline">\(S=X_{0}u\)</span>?</p></li>
</ol>
</body>
</html>
