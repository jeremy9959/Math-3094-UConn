{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Lab\n",
    "\n",
    "In this lab we will apply Principal Components Analysis to the Auto-MPG dataset that we studied in the Chapter on LinearRegression.  Before diving into the real data, we will work with the simulated data from the notes to show how to use python to and numpy to\n",
    "calculate the information we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%setup\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "from holoviews.operation import gridmatrix\n",
    "from bokeh.palettes import Spectral10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated data for demo purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the datamatrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/simulated_pca_data.csv',index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of 200 samples, with 15 features per sample.  To carry out principal component analysis,\n",
    "we must:\n",
    "\n",
    "1. center the data\n",
    "2. compute the covariance matrix D\n",
    "3. find the eigenvectors and eigenvalues of D\n",
    "\n",
    "Then we can:\n",
    "\n",
    "4. project the data into the space spanned by the first two eigenvectors of the covariance matrix and plot this\n",
    "5. draw the loading axes on the plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Centering the data\n",
    "\n",
    "To center the data, we subtract the mean of each column from that column.  The `mean` method computes the mean of each column of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtracting this from the data centers it -- python/pandas understands that when you subtract a scalar from a column, you are really subtracting that scalar from every entry in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_centered = data - data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Computing the Covariance Matrix\n",
    "\n",
    "Remember that the covariance matrix is $X_{0}^{\\intercal}X/N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.dot(data_centered.transpose(),data_centered)/200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.  Finding the eigenvalues and eigenvectors of D\n",
    "\n",
    "The command `np.linalg.eigh` returns a pair consisting of the vector of eigenvalues and the matrix of eigenvectors.\n",
    "By default, the eigenvalues are returned in increasing order, but we like them in decreasing order, so we reverse the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, P = np.linalg.eigh(D)\n",
    "L = L[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalue_plot = figure(title='Eigenvalues of Covariance Matrix')\n",
    "eigenvalue_plot.scatter(x=range(L.shape[0]),y=L,size=8)\n",
    "eigenvalue_plot.line(x=range(L.shape[0]),y=L,color='red')\n",
    "show(eigenvalue_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Projecting the data into the first two principal components\n",
    "\n",
    "The columns of the matrix `P` are the eigenvectors, but they are ordered like the eigenvalues (from smallest to largest).\n",
    "So the two most significant principal components are the *last two* columns of the matrix, and we need to reverse their order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC2 = np.dot(data_centered,P[:,-2::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot = figure(title='Plot of First Two Principal Components',x_range=(-3,3),y_range=(-3,3))\n",
    "scatter_plot.scatter(x=PC2[:,0],y=PC2[:,1])\n",
    "show(scatter_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Draw the loading directions\n",
    "\n",
    "To project the axis of the $i^{th}$ feature into the space spanned by the two principal eigenvectors, we draw a line in the direction of the\n",
    "vector we obtain by multiplying the row vector $[0,\\ldots, 1,\\ldots 0]$, where the $1$ is in position $i$, into that space.  But multiplying\n",
    "that vector times the matrix $P$ just picks out the $i^{th}$ of of $P$, so we want to draw a line in the direction of the point corresponding to the $i^{th}$\n",
    "row of $P$.  For example, the $0^{th}$ feature is in the direction of $(PC[0,0],PC[0,1])$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot.line(x=[-100*PC2[0,0,],100*PC2[0,0]],y=[-100*PC2[0,1],100*PC2[0,1]],color='green')\n",
    "scatter_plot.title.text = 'Plot of First Two Principal Components with Feature 0 Axis'\n",
    "show(scatter_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Auto Data\n",
    "\n",
    "Let's look at what PCA can tell us about the auto data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the data file, and drop the rows with ? for the horsepower\n",
    "data = pd.read_csv('../../data/auto-mpg/auto-mpg.csv',na_values='?')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section on\n",
    "linear regression we explored the relationship between the gas mileage and various other properties of each \n",
    "car model.  We'll continue that analysis from the perspective of principal component analysis in this lab, focusing in particular on:\n",
    "\n",
    "- mileage (mpg)\n",
    "- vehicle weight (weight)\n",
    "- acceleration \n",
    "- horsepower\n",
    "- displacement\n",
    "\n",
    "Because this data is to very different scales, we will not only center it, but rescale it, to make it easier to work with.\n",
    "\n",
    "Display the data so you can see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're only interested in mileage, weight, acceleration, and horsepower in this lab, let's just keep those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the []\n",
    "data = data[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a density plot that shows how the different features are related.  See Figure 9 in the notes for a similar\n",
    "plot of the simulated data that we considered there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_plot = gridmatrix(hv.Dataset(data),chart_type=hv.Points)\n",
    "density_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from this that, for example, increasing horsepower means lower acceleration -- acceleration is measured in time to 60mph,\n",
    "so low numbers correspond to more accleleration.  On the other hand, weight and acceleration, while also somewhat correlated,\n",
    "are less strongly so then weight and engine displacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the steps:\n",
    "\n",
    "1. Center the data (and rescale it)\n",
    "2. Find the covariance matrix\n",
    "3. Compute its eigenvalues and eigenvectors and plot the eigenvalues\n",
    "4. Select the two largest eigenvalues and corresponding eigenvectors\n",
    "5. Draw a scatter plot of the data projected into the span of these two principal directions\n",
    "6. Draw the loadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: center the data and rescale it\n",
    "data_centered = # subtract the mean from each column\n",
    "data_centered = # scale to standard deviation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Find the covariance matrix.  Hint: data.shape[0] is the number of samples\n",
    "D = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Find the eigenvalues and eigenvectors and plot them\n",
    "L, P = #\n",
    "L = #\n",
    "P = 3\n",
    "\n",
    "eigenvalue_plot = figure(title='Eigenvalues')\n",
    "eigenvalue_plot.circle()\n",
    "eigenvalue_plot.line()\n",
    "show(eigenvalue_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Project and plot the data\n",
    "PC2 = #\n",
    "scatter_plot = figure(title='Principal Components')\n",
    "scatter_plot.scatter()\n",
    "show(scatter_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: add the loading directions\n",
    "names = data.columns\n",
    "for i in range(5):\n",
    "    scatter_plot.line(x=#,\n",
    "                      y=#,\n",
    "                      color=Category10[5][i],line_width=3,legend_label=names[i])\n",
    "scatter_plot.title.text = 'Principal Components with Loadings'\n",
    "show(scatter_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In looking at the figure above, notice that weight and mileage are almost perfect opposites -- so there is an unavoidable tradeoff with higher weight vehicles having lower mileage.  Moving to the lower right of the graph, you have better acceleration and also higher horsepower.  Horsepower and displacement point in roughly the same direction, though not perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:\n",
    "\n",
    "- bottom left quadrant are small, high mileage cars with better acceleration\n",
    "- bottom right quadrant are smaller, high-horsepower, fast cars\n",
    "- upper right quadrant are big, heavy, low-mileage cars that are still powerful and relatively fast\n",
    "- upper left quadrant are low acceleration, lower horsepower cars.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
