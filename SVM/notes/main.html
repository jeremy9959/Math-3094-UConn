<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Support Vector Machines</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
  <!-- pandoc-eqnos: equation style -->
  <style>
    .eqnos { display: inline-block; position: relative; width: 100%; }
    .eqnos br { display: none; }
    .eqnos-number { position: absolute; right: 0em; top: 50%; line-height: 0; }
  </style>
</head>
<body>
<h1 data-number="1" id="support-vector-machines"><span class="header-section-number">1</span> Support Vector Machines</h1>
<h2 data-number="1.1" id="introduction"><span class="header-section-number">1.1</span> Introduction</h2>
<p>Suppose that we are given a collection of data made up of samples from two different classes, and we would like to develop an algorithm that can distinguish between the two classes. For example, given a picture that is either a dog or a cat, we’d like to be able to say which of the pictures are dogs, and which are cats. For another example, we might want to be able to distinguish “real” emails from “spam.” This type of problem is called a <em>classification</em> problem.</p>
<p>Typically, one approaches a classification problem by beginning with a large set of data for which you know the classes, and you use that data to <em>train</em> an algorithm to correctly distinguish the classes for the test cases where you already know the answer. For example, you start with a few thousand pictures labelled “dog” and “cat” and you build your algorithm so that it does a good job distinguishing the dogs from the cats in this initial set of <em>training data</em>. Then you apply your algorithm to pictures that aren’t labelled and rely on the predictions you get, hoping that whatever let your algorithm distinguish between the particular examples will generalize to allow it to correctly classify images that aren’t pre-labelled.</p>
<p>Because classification is such a central problem, there are many approaches to it. We will see several of them through the course of these lectures. We will begin with a particular classification algorithm called “Support Vector Machines” (SVM) that is based on linear algebra. The SVM algorithm is widely used in practice and has a beautiful geometric interpretation, so it will serve as a good beginning for later discussion of more complicated classification algorithms.</p>
<p>Incidentally, I’m not sure why this algorithm is called a “machine”; the algorithm was introduced in the paper <span class="citation" data-cites="vapnik92">[<a href="#ref-vapnik92" role="doc-biblioref">1</a>]</span> where it is called the “Optimal Margin Classifier” and as we shall see that is a much better name for it.</p>
<h2 data-number="1.2" id="a-simple-example"><span class="header-section-number">1.2</span> A simple example</h2>
<p>Let us begin our discussion with a very simple dataset (see <span class="citation" data-cites="penguins">[<a href="#ref-penguins" role="doc-biblioref">2</a>]</span> and <span class="citation" data-cites="penguindata">[<a href="#ref-penguindata" role="doc-biblioref">3</a>]</span>). This data consists of various measurements of physical characteristics of 344 penguins of 3 different species: Gentoo, Adelie, and Chinstrap. If we focus our attention for the moment on the Adelie and Gentoo species, and plot their body mass against their culmen depth, we obtain the following scatterplot.</p>
<div id="fig:penguins" class="fignos">
<figure>
<img src="../img/penguins.png" style="width:50.0%" alt="" /><figcaption><span>Figure 1:</span> Penguin Scatterplot</figcaption>
</figure>
</div>
<p>Incidentally, a bird’s <em>culmen</em> is the upper ridge of their beak, and the <em>culmen depth</em> is a measure of the thickness of the beak. There’s a nice picture at <span class="citation" data-cites="penguindata">[<a href="#ref-penguindata" role="doc-biblioref">3</a>]</span> for the penguin enthusiasts.</p>
<p>A striking feature of this scatter plot is that there is a clear separation between the clusters of Adelie and Gentoo penguins. Adelie penguins have deeper culmens and less body mass than Gentoo penguins. These characteristics seem like they should provide a way to classify a penguin between these two species based on these two measurements.</p>
<p>One way to express the separation between these two clusters is to observe that one can draw a line on the graph with the property that all of the Adelie penguins lie on one side of that line and all of the Gentoo penguins lie on the other. In fig. <a href="#fig:penguinsline">2</a> I’ve drawn in such a line (which I found by eyeballing the picture in fig. <a href="#fig:penguins">1</a>). The line has the equation <span class="math display">\[
Y = 250X+400.
\]</span></p>
<div id="fig:penguinsline" class="fignos">
<figure>
<img src="../img/penguins_with_line.png" style="width:50.0%" alt="" /><figcaption><span>Figure 2:</span> Penguins with Separating Line</figcaption>
</figure>
</div>
<p>The fact that all of the Gentoo penguins lie above this line means that, for the Gentoo penguins, their body mass in grams is at least <span class="math inline">\(400\)</span> more than <span class="math inline">\(250\)</span> times their culmen depth in mm.</p>
<p><span class="math display">\[
\mathrm{Gentoo\ mass}&gt; 250(\mathrm{Gentoo\ culmen\ depth})+400
\]</span></p>
<p>while</p>
<p><span class="math display">\[
\mathrm{Adelie\ mass}&lt;250(\mathrm{Adelie\ culmen\ depth})+400.
\]</span></p>
<p>Now, if we measure a penguin caught in the wild, we can compute <span class="math inline">\(250(\mathrm{culmen\ depth})+400\)</span> for that penguin and if this number is greater than the penguin’s mass, we say it’s an Adelie; otherwise, a Gentoo. Based on the experimental data we’ve collected – the <em>training</em> data – this seems likely to work pretty well.</p>
<h2 data-number="1.3" id="the-general-case"><span class="header-section-number">1.3</span> The general case</h2>
<p>To generalize this approach, let’s imagine now that we have <span class="math inline">\(n\)</span> samples and <span class="math inline">\(k\)</span> features (or measurements) for each sample. As before, we can represent this data as an <span class="math inline">\(n\times k\)</span> data matrix <span class="math inline">\(X\)</span>. In the penguin example, our data matrix would be <span class="math inline">\(344\times 2\)</span>, with one row for each penguin and the columns representing the mass and the culmen depth. In addition to this numerical data, we have a classification that assigns each row to one of two classes. Let’s represent the classes by a <span class="math inline">\(n\times 1\)</span> vector <span class="math inline">\(Y\)</span>, where <span class="math inline">\(y_{i}=+1\)</span> if the <span class="math inline">\(i^{th}\)</span> sample is in one class, and <span class="math inline">\(y_{i}=-1\)</span> if that <span class="math inline">\(i^{th}\)</span> sample is in the other. Our goal is to predict <span class="math inline">\(Y\)</span> based on <span class="math inline">\(X\)</span> – but unlike in linear regression, <span class="math inline">\(Y\)</span> takes on the values of <span class="math inline">\(\pm 1\)</span>.</p>
<p>In the penguin case, we were able to find a line that separated the two classes and then classify points by which side of the line the point was on. We can generalize this notion to higher dimensions. Before attacking that generalization, let’s recall a few facts about the generalization to <span class="math inline">\(\mathbf{R}^{k}\)</span> of the idea of a line.</p>
<h3 data-number="1.3.1" id="hyperplanes"><span class="header-section-number">1.3.1</span> Hyperplanes</h3>
<p>The correct generalization of a line given by an equation <span class="math inline">\(w_1 x_1+ w_2 w_2+b=0\)</span> in <span class="math inline">\(\mathbf{R}^{2}\)</span> is an equation <span class="math inline">\(f(x)=0\)</span> where <span class="math inline">\(f(x)\)</span> is a degree one polynomial <span id="eq:degreeone" class="eqnos"><span class="math display">\[
f(x) = f(x_1,\ldots, x_k) = w_1 x_1 + w_2 x_2 +\cdots + w_k x_k + b 
\]</span><span class="eqnos-number">(1)</span></span></p>
<p>It’s easier to understand the geometry of an equation like <span class="math inline">\(f(x)=0\)</span> in eq. <a href="#eq:degreeone">1</a> if we think of the coefficients <span class="math inline">\(w_i\)</span> as forming a <em>nonzero</em> vector <span class="math inline">\(w = (w_1,\ldots, w_k)\)</span> in <span class="math inline">\(\mathbf{R}^{k}\)</span> and writing the formula for <span class="math inline">\(f(x)\)</span> as <span class="math display">\[
f(x) = w\cdot x +b
\]</span>.</p>
<p><strong>Lemma:</strong> Let <span class="math inline">\(f(x)=w\cdot x+b\)</span> with <span class="math inline">\(w\in\mathbf{R}^{k}\)</span> a nonzero vector and <span class="math inline">\(b\)</span> a constant in <span class="math inline">\(\mathbf{R}\)</span>.</p>
<ul>
<li>The inequalities <span class="math inline">\(f(x)&gt;0\)</span> and <span class="math inline">\(f(x)&lt;0\)</span> divide up <span class="math inline">\(\mathbf{R}^{k}\)</span> into two disjoint subsets (called half spaces), in the way that a line in <span class="math inline">\(\mathbf{R}^{2}\)</span> divides the plane in half.</li>
<li>The vector <span class="math inline">\(w\)</span> is normal vector to the hyperplane <span class="math inline">\(f(x)=0\)</span>. Concretely this means that if <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are any two points in that hyperplane, then <span class="math inline">\(w\cdot (p-q)=0\)</span>.</li>
<li>Let <span class="math inline">\(p=(u_1,\ldots,u_k)\)</span> be a point in <span class="math inline">\(\mathbf{R}^{k}\)</span>. Then the perpendicular distance <span class="math inline">\(D\)</span> from <span class="math inline">\(p\)</span> to the hyperplane <span class="math inline">\(f(x)=0\)</span> is <span class="math display">\[
D = \frac{f(p)}{\|w\|}
\]</span></li>
</ul>
<p><strong>Proof:</strong> The first part is clear since the inequalities are mutually exclusive. For the secon part, suppose that <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> satisfy <span class="math inline">\(f(x)=0\)</span>. Then <span class="math inline">\(w\cdot p+b = w\cdot q+b=0\)</span>. Subtracting these two equations gives <span class="math inline">\(w\cdot (p-q)=0\)</span>, so <span class="math inline">\(p-q\)</span> is orthogonal to <span class="math inline">\(w\)</span>.</p>
<p>For the third part, consider fig. <a href="#fig:triangle">3</a>. The point <span class="math inline">\(q\)</span> is an arbitrary point on the hyperplane defined by the equation <span class="math inline">\(w\cdot x+b=0\)</span>. The distance from the hyperplane to <span class="math inline">\(p\)</span> is measured along the dotted line perpendicular to the hyperplane. The dot product <span class="math inline">\(w\cdot (p-q) = \|w\|\|p-q\|\cos(\theta)\)</span> where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(p-q\)</span> and <span class="math inline">\(w\)</span> – which is complementary to the angle between <span class="math inline">\(p-q\)</span> and the hyperplane. The distance <span class="math inline">\(D\)</span> is therefore <span class="math display">\[
D=\frac{w\cdot(p-q)}{\|w\|}.
\]</span> However, since <span class="math inline">\(q\)</span> lies on the hyperplane, we know that <span class="math inline">\(w\cdot q+b=0\)</span> so <span class="math inline">\(w\cdot q = -b\)</span>. Therefore <span class="math inline">\(w\cdot(p-q)=w\cdot p+b=f(p)\)</span>, which is the formula we seek.</p>
<div id="fig:triangle" class="fignos">
<figure>
<img src="../img/triangle.png" style="width:30.0%" alt="" /><figcaption><span>Figure 3:</span> Distance to a Hyperplane</figcaption>
</figure>
</div>
<h3 data-number="1.3.2" id="sec:linearseparable"><span class="header-section-number">1.3.2</span> Linear separability</h3>
<p>Now we can return to our classification scheme. The following definition generalizes our two dimensional picture from the penguin data.</p>
<p><strong>Definition:</strong> Suppose that we have an <span class="math inline">\(n\times k\)</span> data matrix <span class="math inline">\(X\)</span> and a set of labels <span class="math inline">\(Y\)</span> that assign the <span class="math inline">\(n\)</span> samples to one of two classes. Then the labelled data is said to be <em>linearly separable</em> if there is a vector <span class="math inline">\(w\)</span> and a constant <span class="math inline">\(b\)</span> so that, if <span class="math inline">\(f(x)=w\cdot x+b\)</span>, then <span class="math inline">\(f(x)&gt;0\)</span> whenever <span class="math inline">\(x=(x_1,\ldots, x_k)\)</span> is a row of <span class="math inline">\(X\)</span> – a sample – belonging to the <span class="math inline">\(+1\)</span> class, and <span class="math inline">\(f(x)&lt;0\)</span> whenever <span class="math inline">\(x\)</span> belongs to the <span class="math inline">\(-1\)</span> class. The solutions to the equation <span class="math inline">\(f(x)=0\)</span> in this situation form a hyperplane that is called a <em>separating hyperplane</em> for the data.</p>
<p>In the situation where our data falls into two classes that are linearly separable, our classification strategy is to find a separating hyperplane <span class="math inline">\(f\)</span> for our training data. Then, given a point <span class="math inline">\(x\)</span> whose class we don’t know, we can evaluate <span class="math inline">\(f(x)\)</span> and assign <span class="math inline">\(x\)</span> to a class depending on whether <span class="math inline">\(f(x)&gt;0\)</span> or <span class="math inline">\(f(x)&lt;0\)</span>.</p>
<p>This definition begs two questions about a particular dataset:</p>
<ol type="1">
<li>How do we tell if the two classes are linearly separable?</li>
<li>If the two sets are linearly separable, there are infinitely many separating hyperplanes. To see this, look back at the penguin example and notice that we can ‘wiggle’ the red line a little bit and it will still separate the two sets. Which is the ‘best’ separating hyperplane?</li>
</ol>
<p>Let’s try to make the first of these two questions concrete. We have two sets of points <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> in <span class="math inline">\(\mathbf{R}^{k}\)</span>, and we want to (try to) find a vector <span class="math inline">\(w\)</span> and a constant <span class="math inline">\(b\)</span> so that <span class="math inline">\(f(x)=w\cdot x+b\)</span> takes strictly positive values for <span class="math inline">\(x\in A\)</span> and strictly negative ones for <span class="math inline">\(x\in B\)</span>. Let’s approach the problem by first choosing <span class="math inline">\(w\)</span> and then asking whether there is a <span class="math inline">\(b\)</span> that will work. In the two dimensional case, this is equivalent to choosing the slope of our line, and then asking if we can find an intercept so that the line passes between the two classes.</p>
<p>In algebraic terms, we are trying to solve the following system of inequalities: given <span class="math inline">\(w\)</span>, find <span class="math inline">\(b\)</span> so that: <span class="math display">\[
w\cdot x+b&gt;0 \hbox{ for all $x$ in A}
\]</span> and <span class="math display">\[
w\cdot x+b&lt;0\hbox{ for all $x$ in B}.
\]</span> This is only going to be possible if there is a gap between the smallest value of <span class="math inline">\(w\cdot x\)</span> for <span class="math inline">\(x\in A\)</span> and the largest value of <span class="math inline">\(w\cdot x\)</span> for <span class="math inline">\(x\in B\)</span>. In other words, given <span class="math inline">\(w\)</span> there is a <span class="math inline">\(b\)</span> so that <span class="math inline">\(f(x)=w\cdot x+b\)</span> separates <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> if <span class="math display">\[
\max_{x\in B}w\cdot x &lt; \min_{x\in A} w\cdot x.
\]</span> If this holds, then choose <span class="math inline">\(b\)</span> so that <span class="math inline">\(-b\)</span> lies in this open interval and you will obtain a separating hyperplane.</p>
<p><strong>Proposition:</strong> The sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are linearly separable if there is a <span class="math inline">\(w\)</span> so that <span class="math display">\[
\max_{x\in B}w\cdot x &lt; \min_{x\in A} w\cdot x
\]</span> If this inequality holds for some <span class="math inline">\(w\)</span>, and <span class="math inline">\(-b\)</span> within this open interval, then <span class="math inline">\(f(x)=w\cdot x+b\)</span> is a separating hyperplane for <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<p>Figure <a href="#fig:penguinhwy2">4</a> is an illustration of this argument for a subset of the penguin data. Here, we have fixed <span class="math inline">\(w=(250,-1)\)</span> coming from the line <span class="math inline">\(y=250x+400\)</span> that we eyeballed earlier. For each Gentoo (green) point <span class="math inline">\(x_{i}\)</span>, we computed <span class="math inline">\(-b=w\cdot x_{i}\)</span> and drew the line <span class="math inline">\(f(x) = w\cdot x - w\cdot x_{i}\)</span> giving a family of parallel lines through each of the green points. Similarly for each Adelie (blue) point we drew the corresponding line. The maximum value of <span class="math inline">\(w\cdot x\)</span> for the blue points turned out to be <span class="math inline">\(-75\)</span> and the minimum value of <span class="math inline">\(w\cdot x\)</span> for the green points turned out to be <span class="math inline">\(525\)</span>. Thus we have two lines with a gap between them, and any parallel line in that gap will separate the two sets.</p>
<p>Finally, among all the lines with this particular <span class="math inline">\(w\)</span>, it seems that the <strong>best</strong> separating line is the one running right down the middle of the gap between the boundary lines. Any other line in the gap will be closer to either the blue or green set that the midpoint line is.</p>
<div id="fig:penguinhwy2" class="fignos">
<figure>
<img src="../img/penguinhwy2.png" style="width:50.0%" alt="" /><figcaption><span>Figure 4:</span> Supporting lines in Penguin Data</figcaption>
</figure>
</div>
<p>Let’s put all of this together and see if we can make sense of it in general.</p>
<p>Suppose that <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> are finite point sets in <span class="math inline">\(\mathbf{R}^{k}\)</span> and <span class="math inline">\(w\in\mathbf{R}^{k}\)</span> such that <span class="math display">\[
B^{-}=\max_{x\in A^{-}}w\cdot x &lt; \min_{x\in A^{+}}w\cdot x=B^{+}.
\]</span> Let <span class="math inline">\(x^{-}\)</span> be a point in <span class="math inline">\(A^{-}\)</span> with <span class="math inline">\(w\cdot x^{-}=B^{-}\)</span> and <span class="math inline">\(x^{+}\)</span> be a point in <span class="math inline">\(A\)</span> with <span class="math inline">\(w\cdot x^{+}=B^{+}\)</span>. The two hyperplanes <span class="math inline">\(f^{\pm}(x) = w\cdot x - B^{\pm}\)</span> have the property that: <span class="math display">\[
f^{+}(x)\ge 0\hbox{ for }x\in A^{+}\hbox{ and }f^{+}(x)&lt;0\hbox{ for }x\in A^{-}
\]</span> and <span class="math display">\[
f^{-}(x)\le 0\hbox{ for }x\in A^{-}\hbox{ and }f^{-}(x)&gt;0\hbox{ for }x\in A^{+}
\]</span></p>
<p>Hyperplanes like <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span>, which “just touch” a set of points, are called supporting hyperplanes.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(A\)</span> be a set of points in <span class="math inline">\(\mathbf{R}^{k}\)</span>. A hyperplane <span class="math inline">\(f(x)=w\cdot x+b=0\)</span> is called a <em>supporting hyperplane</em> for <span class="math inline">\(A\)</span> if <span class="math inline">\(f(x)\ge 0\)</span> for all <span class="math inline">\(x\in A\)</span> and <span class="math inline">\(f(x)=0\)</span> for at least one point in <span class="math inline">\(A\)</span>, or if <span class="math inline">\(f(x)\le 0\)</span> for all <span class="math inline">\(x\in A\)</span> and <span class="math inline">\(f(x)=0\)</span> for at least one point in <span class="math inline">\(A\)</span>.</p>
<p>The gap between the two supporting hyperplanes <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span> is called the <em>margin</em> between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> for <span class="math inline">\(w\)</span>.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span> be as in the discussion above for point sets <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> and vector <span class="math inline">\(w\)</span>. Then the orthogonal distance between the two hyperplanes <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span> is called the geometric margin <span class="math inline">\(\tau_{w}(A^{+},A^{-})\)</span> (along <span class="math inline">\(w\)</span>) between <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>. We have <span class="math display">\[
\tau_{w}(A^{+},A^{-})=\frac{|B^{+}-B^{-}|}{\|w\|}.
\]</span></p>
<p><strong>Definition:</strong> The <em>optimal margin</em> <span class="math inline">\(\tau(A^{+},A^{-})\)</span> between <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> is the largest value of <span class="math inline">\(\tau_{w}\)</span> over all possible <span class="math inline">\(w\)</span>: <span class="math display">\[
\tau(A^{+},A^{-}) = \max_{w} \tau_{w}(A^{+},A^{-}).
\]</span> If <span class="math inline">\(w\)</span> is such that <span class="math inline">\(\tau_{w}=\tau\)</span>, then the hyperplane <span class="math inline">\(f(x)=w\cdot x - \frac{(B^{+}+B^{-})}{2}\)</span> is the yields the <em>optimal margin classifying hyperplane</em>.</p>
<p>The optimal classifying hyperplane runs “down the middle” of the gap between the two supporting hyperplanes <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span> that give the sides of the optimal margin.</p>
<p>Our task is to find a way to determine the optimal margin. To do so, we need a digression to discuss a key property known as convexity.</p>
<h2 data-number="1.4" id="convexity-convex-hulls-and-margins"><span class="header-section-number">1.4</span> Convexity, Convex Hulls, and Margins</h2>
<p>In this section we introduce the notion of a <em>convex set</em> and the particular case of the <em>convex hull</em> of a finite set of points. As we will see, these ideas will give us a different interpretation of the margin between two sets and will eventually lead to an algorithm for finding the optimal margin classifier.</p>
<p><strong>Definition:</strong> A subset <span class="math inline">\(U\)</span> of <span class="math inline">\(\mathbf{R}^{k}\)</span> is <em>convex</em> if, for any pair of points <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> in <span class="math inline">\(U\)</span>, every point <span class="math inline">\(t\)</span> on the line segment joining <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> also belongs to <span class="math inline">\(U\)</span>. In vector form, for every <span class="math inline">\(0\le s\le 1\)</span>, the point <span class="math inline">\(t(s) = sp+(1-s)q\)</span> belongs to <span class="math inline">\(U\)</span>.</p>
<p>Figure <a href="#fig:convexnotconvex">5</a> illustrates the difference between convex sets and non-convex ones.</p>
<div id="fig:convexnotconvex" class="fignos">
<figure>
<img src="../img/ConvexNotConvex.png" style="width:50.0%" alt="" /><figcaption><span>Figure 5:</span> Convex vs Non-Convex Sets</figcaption>
</figure>
</div>
<h1 class="unnumbered" data-number="" id="bibliography">References</h1>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-vapnik92">
<p>[1] <span class="smallcaps">Boser</span>, B., <span class="smallcaps">Guyon</span>, I. and <span class="smallcaps">Vapnik</span>, V. A training algorithm for optimal margin classifiers. In <em>Colt ’92: Proceedings of the fifth annual workshop on computational learning theory</em> (D. Haussler, ed) pp 144–52. ACM.</p>
</div>
<div id="ref-penguins">
<p>[2] <span class="smallcaps">KB</span>, G., <span class="smallcaps">TD</span>, W. and <span class="smallcaps">WR</span>, F. (2014). Ecological sexual dimorphism and environmental variability within a community of antarctic penguins (genus pygoscelis). <em>PLoS ONE</em> <strong>9(3)</strong> –13.</p>
</div>
<div id="ref-penguindata">
<p>[3] <span class="smallcaps">Horst</span>, A. Palmer penguins.Available at <a href="https://https://github.com/allisonhorst/palmerpenguins">https://https://github.com/allisonhorst/palmerpenguins</a>.</p>
</div>
</div>
</body>
</html>
