<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Support Vector Machines</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 data-number="1" id="support-vector-machines"><span class="header-section-number">1</span> Support Vector Machines</h1>
<h2 data-number="1.1" id="introduction"><span class="header-section-number">1.1</span> Introduction</h2>
<p>Suppose that we are given a collection of data made up of samples from two different classes, and we would like to develop an algorithm that can distinguish between the two classes. For example, given a picture that is either a dog or a cat, we’d like to be able to say which of the pictures are dogs, and which are cats. For another example, we might want to be able to distinguish “real” emails from “spam.” This type of problem is called a <em>classification</em> problem.</p>
<p>Typically, one approaches a classification problem by beginning with a large set of data for which you know the classes, and you use that data to <em>train</em> an algorithm to correctly distinguish the classes for the test cases where you already know the answer. For example, you start with a few thousand pictures labelled “dog” and “cat” and you build your algorithm so that it does a good job distinguishing the dogs from the cats in this initial set of <em>training data</em>. Then you apply your algorithm to pictures that aren’t labelled and rely on the predictions you get, hoping that whatever let your algorithm distinguish between the particular examples will generalize to allow it to correctly classify images that aren’t pre-labelled.</p>
<p>Because classification is such a central problem, there are many approaches to it. We will see several of them through the course of these lectures. We will begin with a particular classification algorithm called “Support Vector Machines” (SVM) that is based on linear algebra. The SVM algorithm is widely used in practice and has a beautiful geometric interpretation, so it will serve as a good beginning for later discussion of more complicated classification algorithms.</p>
<p>Incidentally, I’m not sure why this algorithm is called a “machine”; the algorithm was introduced in the paper <span class="citation" data-cites="vapnik92">[<a href="#ref-vapnik92" role="doc-biblioref">1</a>]</span> where it is called the “Optimal Margin Classifier” and as we shall see that is a much better name for it.</p>
<h2 data-number="1.2" id="a-simple-example"><span class="header-section-number">1.2</span> A simple example</h2>
<p>Let us begin our discussion with a very simple dataset (see <span class="citation" data-cites="penguins">[<a href="#ref-penguins" role="doc-biblioref">2</a>]</span> and <span class="citation" data-cites="penguindata">[<a href="#ref-penguindata" role="doc-biblioref">3</a>]</span>). This data consists of various measurements of physical characteristics of 344 penguins of 3 different species: Gentoo, Adelie, and Chinstrap. If we focus our attention for the moment on the Adelie and Gentoo species, and plot their body mass against their culmen depth, we obtain the following scatterplot.</p>
<div id="fig:penguins" class="fignos">
<figure>
<img src="../img/penguins.png" style="width:50.0%" alt="" /><figcaption><span>Figure 1:</span> Penguin Scatterplot</figcaption>
</figure>
</div>
<p>Incidentally, a bird’s <em>culmen</em> is the upper ridge of their beak, and the <em>culmen depth</em> is a measure of the thickness of the beak. There’s a nice picture at <span class="citation" data-cites="penguindata">[<a href="#ref-penguindata" role="doc-biblioref">3</a>]</span> for the penguin enthusiasts.</p>
<p>A striking feature of this scatter plot is that there is a clear separation between the clusters of Adelie and Gentoo penguins. Adelie penguins have deeper culmens and less body mass than Gentoo penguins. These characteristics seem like they should provide a way to classify a penguin between these two species based on these two measurements.</p>
<p>One way to express the separation between these two clusters is to observe that one can draw a line on the graph with the property that all of the Adelie penguins lie on one side of that line and all of the Gentoo penguins lie on the other. In fig. <a href="#fig:penguinsline">2</a> I’ve drawn in such a line (which I found by eyeballing the picture in fig. <a href="#fig:penguins">1</a>). The line has the equation <span class="math display">\[
Y = 250X+400.
\]</span></p>
<div id="fig:penguinsline" class="fignos">
<figure>
<img src="../img/penguins_with_line.png" style="width:50.0%" alt="" /><figcaption><span>Figure 2:</span> Penguins with Separating Line</figcaption>
</figure>
</div>
<p>The fact that all of the Gentoo penguins lie above this line means that, for the Gentoo penguins, their body mass in grams is at least <span class="math inline">\(400\)</span> more than <span class="math inline">\(250\)</span> times their culmen depth in mm.</p>
<p><span class="math display">\[
\mathrm{Gentoo\ mass}&gt; 250(\mathrm{Gentoo\ culmen\ depth})+400
\]</span></p>
<p>while</p>
<p><span class="math display">\[
\mathrm{Adelie\ mass}&lt;250(\mathrm{Adelie\ culmen\ depth})+400.
\]</span></p>
<p>Now, if we measure a penguin caught in the wild, we can compute <span class="math inline">\(250(\mathrm{culmen\ depth})+400\)</span> for that penguin and if this number is greater than the penguin’s mass, we say it’s an Adelie; otherwise, a Gentoo. Based on the experimental data we’ve collected – the <em>training</em> data – this seems likely to work pretty well.</p>
<h2 data-number="1.3" id="the-general-case"><span class="header-section-number">1.3</span> The general case</h2>
<p>To generalize this approach, let’s imagine now that we have <span class="math inline">\(n\)</span> samples and <span class="math inline">\(k\)</span> features (or measurements) for each sample. As before, we can represent this data as an <span class="math inline">\(n\times k\)</span> data matrix <span class="math inline">\(X\)</span>. In the penguin example, our data matrix would be <span class="math inline">\(344\times 2\)</span>, with one row for each penguin and the columns representing the mass and the culmen depth. In addition to this numerical data, we have a classification that assigns each row to one of two classes. Let’s represent the classes by a <span class="math inline">\(n\times 1\)</span> vector <span class="math inline">\(Y\)</span>, where <span class="math inline">\(y_{i}=+1\)</span> if the <span class="math inline">\(i^{th}\)</span> sample is in one class, and <span class="math inline">\(y_{i}=-1\)</span> if that <span class="math inline">\(i^{th}\)</span> sample is in the other. Our goal is to predict <span class="math inline">\(Y\)</span> based on <span class="math inline">\(X\)</span> – but unlike in linear regression, <span class="math inline">\(Y\)</span> takes on the values of <span class="math inline">\(\pm 1\)</span>.</p>
<p>In the penguin case, we were able to find a line that separated the two classes. We can generalize this notion to higher dimensions.</p>
<p><strong>Definition:</strong> Suppose that we have an <span class="math inline">\(n\times k\)</span> data matrix <span class="math inline">\(X\)</span> and a set of labels <span class="math inline">\(Y\)</span> that assign the <span class="math inline">\(n\)</span> samples to one of two classes. Then the labelled data is said to be <em>linearly separable</em> if there is a degree one polynomial <span class="math display">\[
f(x) = f(x_1,\ldots, x_k) = a_1 x_1 + a_2 x_2 +\cdots + a_k x_k + b
\]</span> so that <span class="math inline">\(f(x)&gt;0\)</span> whenever <span class="math inline">\(x=(x_1,\ldots, x_k)\)</span> is a row of <span class="math inline">\(X\)</span> – a sample – belonging to the <span class="math inline">\(+1\)</span> class, and <span class="math inline">\(f(x)&lt;0\)</span> whenever <span class="math inline">\(x\)</span> belongs to the <span class="math inline">\(-1\)</span> class. A function <span class="math inline">\(f\)</span> that satisfies this property is called a <em>separating hyperplane</em>.</p>
<p>In <span class="math inline">\(\mathbf{R}^{k}\)</span>, the set of points <span class="math inline">\(x\)</span> where <span class="math inline">\(f(x)=0\)</span> is a hyperplane, and the regions where <span class="math inline">\(f(x)&gt;0\)</span> and <span class="math inline">\(f(x)&lt;0\)</span> are the two “sides” of that hyperplane. So the definition says that two sets are linearly separable if we can find a hyperplane so that each set lies on a different side of that hyperplane.</p>
<p>Our classification strategy, then, is to find a separating hyperplane <span class="math inline">\(f\)</span> for our training data. Then, given a point <span class="math inline">\(x\)</span> whose class we don’t know, we can evaluate <span class="math inline">\(f(x)\)</span> and assign <span class="math inline">\(x\)</span> to a class depending on whether <span class="math inline">\(f(x)&gt;0\)</span> or <span class="math inline">\(f(x)&lt;0\)</span>.</p>
<p>This definition begs two questions about a particular dataset:</p>
<ol type="1">
<li>How do we tell if the two classes are linearly separable?</li>
<li>If the two sets are linearly separable, there are infinitely many separating hyperplanes. To see this, look back at the penguin example and notice that we can ‘wiggle’ the red line a little bit and it will still separate the two sets. Which is the ‘best’ separating hyperplane?</li>
</ol>
<p>For the moment, we will table the first question, and <em>assume</em> that our data belongs to two classes that are linearly separable, and we will look at one possible answer to the second question: what’s the best separating hyperplane?</p>
<h2 data-number="1.4" id="margins"><span class="header-section-number">1.4</span> Margins</h2>
<p>Let’s look at a simplified version of the penguin data, where we have two linearly separable clusters of points in the plane. In fig. <a href="#fig:penguinsimple">3</a> we show a plot of a randomly selected subset of the penguin points, along with the line <span class="math inline">\(Y=250X+400\)</span> that we eyeballed earlier.</p>
<div id="fig:penguinsimple" class="fignos">
<figure>
<img src="../img/penguinsimple.png" style="width:50.0%" alt="" /><figcaption><span>Figure 3:</span> A subset of penguins</figcaption>
</figure>
</div>
<p>One way to measure how well this line separates the two clusters is to ask how close the nearest point in each cluster is to the line. Looking at fig. <a href="#fig:penguinsimple">3</a>, you’ll notice that the line is closer to the green (Gentoo) cluster than it is to the blue (Adelie) cluster. So, at least as far as this subset of the data is concerned, we could get better separation if we moved the line downwards.</p>
<p>Of course, we could also tilt the line by changing its slope, and perhaps if we increased the slope a bit, that would get us better separation as well.</p>
<p>In any case, this suggests that we measure the effectiveness of our line at separating the two clusters by <em>looking at the distance from the line to the closest point(s) in each cluster, and trying to make those distances as large as possible.</em></p>
<p>The following lemma reminds us of a few facts about hyperplanes in <span class="math inline">\(\mathbf{R}^{k}\)</span>, including how to compute the distance from a point to a line (or to a hyperplane in higher dimensions).</p>
<p><strong>Lemma:</strong> Let <span class="math inline">\(f(x_1,\ldots, x_k) = \sum_{i=1}^{k} a_i x_i +b\)</span> with not all <span class="math inline">\(a_i=0\)</span>. Let <span class="math inline">\(w=(a_1,\ldots,a_k)\)</span> viewed as a vector in <span class="math inline">\(\mathbf{R}^{k}\)</span>, so that if we view <span class="math inline">\(x=(x_1,\ldots, x_k)\)</span> as a vector we can write a vector version of <span class="math inline">\(f\)</span>: <span class="math display">\[
f(x)=w\cdot x+b.
\]</span></p>
<ul>
<li>The vector <span class="math inline">\(w\)</span> is normal vector to the hyperplane <span class="math inline">\(f(x)=0\)</span>. Concretely this means that if <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are any two points in that hyperplane, then <span class="math inline">\(w\cdot (p-q)=0\)</span>.</li>
<li>Let <span class="math inline">\(p=(u_1,\ldots,u_k)\)</span> be a point in <span class="math inline">\(\mathbf{R}^{k}\)</span>. Then the perpendicular distance <span class="math inline">\(D\)</span> from <span class="math inline">\(p\)</span> to the hyperplane <span class="math inline">\(f(x)=0\)</span> is <span class="math display">\[
D = \frac{f(p)}{\|w\|}
\]</span></li>
</ul>
<h1 class="unnumbered" data-number="" id="bibliography">References</h1>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-vapnik92">
<p>[1] <span class="smallcaps">Boser</span>, B., <span class="smallcaps">Guyon</span>, I. and <span class="smallcaps">Vapnik</span>, V. A training algorithm for optimal margin classifiers. In <em>Colt ’92: Proceedings of the fifth annual workshop on computational learning theory</em> (D. Haussler, ed) pp 144–52. ACM.</p>
</div>
<div id="ref-penguins">
<p>[2] <span class="smallcaps">KB</span>, G., <span class="smallcaps">TD</span>, W. and <span class="smallcaps">WR</span>, F. (2014). Ecological sexual dimorphism and environmental variability within a community of antarctic penguins (genus pygoscelis). <em>PLoS ONE</em> <strong>9(3)</strong> –13.</p>
</div>
<div id="ref-penguindata">
<p>[3] <span class="smallcaps">Horst</span>, A. Palmer penguins.Available at <a href="https://https://github.com/allisonhorst/palmerpenguins">https://https://github.com/allisonhorst/palmerpenguins</a>.</p>
</div>
</div>
</body>
</html>
