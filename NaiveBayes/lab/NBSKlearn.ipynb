{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instrumental-denial",
   "metadata": {},
   "source": [
    "# Naive Bayes Sentiment Analysis\n",
    "\n",
    "## Setup\n",
    "\n",
    "In this lab we will take advantage of some parts of the scikit-learn (sklearn) machine learning library to carry out a Naive Bayes\n",
    "sentiment analysis of some amazon product reviews.\n",
    "\n",
    "In addition to our usual libraries (numpy for linear algebra and bokeh for plotting) we load two functions\n",
    "from sklearn.\n",
    "\n",
    "- ```CountVectorizer``` extracts wordcounts from documents\n",
    "- ```train_test_split``` splits our data up into a \"training set\" from which we will derive our probabilities, and \n",
    "    a \"test\" set that we will use to evaluate our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "asian-principle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"p1001\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"p1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.0.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.0.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.0.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.0.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.0.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"p1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook\n",
    "import numpy as np\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-malawi",
   "metadata": {},
   "source": [
    "First we read our data from the file.  Each line of the file consists of a review, followed by a tab character,\n",
    "followed by a \"0\" or \"1\".  We build a list of reviews and a corresponding list of labels from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "convertible-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "labels = []\n",
    "with open(\"amazon.txt\") as f:\n",
    "    for line in f:\n",
    "        review, label = line.strip().split('\\t')\n",
    "        reviews.append(review)\n",
    "        labels.append(int(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-stationery",
   "metadata": {},
   "source": [
    "The train_test_split breaks up the reviews and labels array randomly into two parts; by default, 75% of the data\n",
    "goes into the train set and 25% into the test set, though this is adjustable. Now we set aside the test data\n",
    "and work only with the training data until the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "allied-nightmare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_reviews is  750\n",
      "Length of test_reviews is  250\n"
     ]
    }
   ],
   "source": [
    "train_reviews, test_reviews, train_labels, test_labels = train_test_split(reviews, labels,random_state=11)\n",
    "print('Length of train_reviews is ', len(train_reviews))\n",
    "print('Length of test_reviews is ',len(test_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-nudist",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "Now we use the CountVectorizer function to analyze our reviews.  The syntax here is that we create a CountVectorizer object and apply it to our review data.  We set options to say that  we only want to keep track of the 100 most common words in the reviews and (using ```binary=True```)that we only want to mark words that occur with a zero or 1 -- otherwise the routine will count the number of occurrences of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comic-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = CountVectorizer(max_features=100,binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-westminster",
   "metadata": {},
   "source": [
    "To see how this works, let's apply it to a couple of simple sentences. The vectorizer expects a list of sentences, so we'll give a list of three sentences.  It returns a matrix whose rows correspond to the sentences and whose columns are features corresponding to the words it discovered in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "threaded-highland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple = F.fit_transform(['This is a simple sentence that contains the word sentence twice.' ,\n",
    "                          'This is another sentence.',\n",
    "                          'A simple sentence has twice.'])\n",
    "simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-closer",
   "metadata": {},
   "source": [
    "The vectorizer returns a \"sparse\" array, which is an efficient way to store large matrices which are mostly zero.  We'll\n",
    "see how to view it in a moment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-religion",
   "metadata": {},
   "source": [
    "First, we can ask the vectorizer for the vocabulary that it uncovered.  It returns a python dictionary that associates\n",
    "words to columns.  So for example in this case the word ```word``` corresponds to the 9th column of the data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fuzzy-thread",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'simple': 5,\n",
       " 'sentence': 4,\n",
       " 'that': 6,\n",
       " 'contains': 1,\n",
       " 'the': 7,\n",
       " 'word': 10,\n",
       " 'twice': 9,\n",
       " 'another': 0,\n",
       " 'has': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-hacker",
   "metadata": {},
   "source": [
    "Now the array ```simple```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "special-anatomy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-yugoslavia",
   "metadata": {},
   "source": [
    "Each row is the feature vector to a sentence, and each column corresponds to a  word.  So the first sentence does *not* contain the first key word (```another```) but the second one does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-entrance",
   "metadata": {},
   "source": [
    "The column sums tell us how often each word occurs (total) in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "existing-indie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 2, 3, 2, 1, 1, 2, 2, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-berlin",
   "metadata": {},
   "source": [
    "Suppose the first sentence is \"positive\" (labelled with 1) and the other two are negative (labelled with zero). The corresponding target array is Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "controversial-marathon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([[1],[0],[0]])\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-artwork",
   "metadata": {},
   "source": [
    "We can compute the frequencies in the positive documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "blank-graphics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.transpose() @ simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-israeli",
   "metadata": {},
   "source": [
    "and in the negative ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "approximate-doctrine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 2, 1, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-Y).transpose() @ simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-lemon",
   "metadata": {},
   "source": [
    "The fourth word is 'sentence' which does indeed occur once in the type 1 sentences and 2 times in the type 0 ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-devon",
   "metadata": {},
   "source": [
    "The numbers of sentences of each type are $Y^TY$ and $(1-Y)^T(1-Y)$ although numpy thinks these are two dimensional 1x1 arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "subject-substitute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.transpose()@Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "excess-family",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-Y).transpose()@(1-Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-essex",
   "metadata": {},
   "source": [
    "We can compute the conditional probabilities with which each word occurs in the two types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "instant-slovakia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pplus = Y.transpose()@simple/(Y.transpose()@Y)\n",
    "Pplus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-opposition",
   "metadata": {},
   "source": [
    "Just as a check, the first word, ```another```, occurs only in the second sentence, so if has a 50% chance of\n",
    "occurring in a sentence labelled zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "comfortable-relative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0. , 0.5, 0.5, 1. , 0.5, 0. , 0. , 0.5, 0.5, 0. ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pminus = (1-Y).transpose()@simple/((1-Y).transpose()@(1-Y))\n",
    "Pminus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-assist",
   "metadata": {},
   "source": [
    "Now let's look at our training data.  First we use the CountVectorizer to compute the feature matrix.  We're going to add an option \n",
    "to tell the vectorizer to ignore elements of a list of \"stop words\" like he, his, at, him, ... to simplify things. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-indonesian",
   "metadata": {},
   "source": [
    "## The product review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=100,binary=True,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = vectorizer.fit_transform(train_reviews).toarray()\n",
    "keywords = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-magazine",
   "metadata": {},
   "source": [
    "Let's compute the frequencies of the 100 words in each of the two classes.  We convert our labels into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-network",
   "metadata": {},
   "source": [
    "We use our formulae to compute the frequencies and the conditional probabilitiy vectors for the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_plus = (train_y.transpose()@train_matrix)\n",
    "Nplus = train_y.transpose()@train_y\n",
    "Pplus = freq_plus/Nplus\n",
    "freq_minus = ((1-train_y).transpose()@train_matrix)\n",
    "Nminus = ((1-train_y).transpose()@(1-train_y))\n",
    "Pminus = freq_minus/Nminus\n",
    "N = Nminus+Nplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Nplus, Nminus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-costa",
   "metadata": {},
   "source": [
    "Here we use a trick called \"indirect sort\" to find the words with largest P(w|+) and P(w|-).  Argsort returns this *locations* of\n",
    "the elements in order.  So indices[0] is the location in the Pplus array with the smallest value, indices[1] the next smallest value, and so on.\n",
    "We use these indices to extract the corresponding keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(Pplus)\n",
    "[keywords[i] for i in indices[::-1]][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(Pminus)\n",
    "[keywords[i] for i in indices[::-1]][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a fancy use of bokeh to add hover labels to the dots\n",
    "source = ColumnDataSource({'+':Pplus,'-':Pminus,'word':keywords})\n",
    "f=figure()\n",
    "f.scatter(x='+',y='-',source=source)\n",
    "\n",
    "f.xaxis.axis_label='P(w|+)'\n",
    "f.yaxis.axis_label = 'P(w|-)'\n",
    "f.line(x=[0,.2],y=[0,.2])\n",
    "f.add_tools(HoverTool(tooltips=[(\"word\",\"@word\")]))\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-siemens",
   "metadata": {},
   "source": [
    "To avoid taking the logarithm of zero, we increase all of the frequency counts by 1, as well as the Nplus and Nminus by 1. This is often called\n",
    "\"smoothing.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_plus = freq_plus+1\n",
    "freq_minus = freq_minus+1\n",
    "Nplus = Nplus+2\n",
    "Nminus = Nminus+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "LPplus = np.log(freq_plus/Nplus)\n",
    "LPNplus = np.log(1-freq_plus/Nplus)\n",
    "LPminus = np.log(freq_minus/Nminus)\n",
    "LPNminus = np.log(1-freq_minus/Nminus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-psychology",
   "metadata": {},
   "source": [
    "Using the equation from the notes (which is essentially Bayes rule), we find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "posL = train_matrix @ LPplus + (1-train_matrix) @(LPNplus) - np.log(Nplus/(N+2))\n",
    "negL  = train_matrix @ LPminus + (1-train_matrix)@(LPNminus) - np.log(Nminus/(N+2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-junction",
   "metadata": {},
   "source": [
    "Recall that posL and negL are the likelihoods that a particular review is positive or negative, and our decision criterion is:\n",
    "- label 1 if posL-negL>0\n",
    "- label 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-blair",
   "metadata": {},
   "source": [
    "Our decision array has a 1 if posL>negL and a zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = (posL > negL).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-ratio",
   "metadata": {},
   "source": [
    "Our check array as a 1 if decision and the original label agree, and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = (decision==train_y).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-intake",
   "metadata": {},
   "source": [
    "They agree 591/750 times, or about 75% of the time.  That's much better than guessing, which would only be right 50% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-improvement",
   "metadata": {},
   "source": [
    "Finally, we use the test data to see if we can predict labels on \"new\" data.\n",
    "We re-use the LPplus and LPminus parameters, as well as the Nplus/N and Nminus/N from the training data.\n",
    "But we need to compute the data matrix for the test data *based on the features derived from the training data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(train_reviews)\n",
    "test_matrix = vectorizer.transform(test_reviews).toarray()\n",
    "test_y = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "posL = test_matrix @ LPplus + (1-test_matrix)@(LPNplus) -np.log(Nplus/(Nplus+Nminus))\n",
    "negL = test_matrix @ LPminus + (1-test_matrix)@(LPNminus) - np.log(Nminus/(Nplus+Nminus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decision = (posL > negL).astype(int)\n",
    "check = (test_decision == test_y).astype(int)\n",
    "np.sum(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-cache",
   "metadata": {},
   "source": [
    "So we have correctly classified 182/250 reviews from the test set for an accuracy of 171/250 = 73%. Much better than guessing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-phoenix",
   "metadata": {},
   "source": [
    "## Using the sklearn facilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-douglas",
   "metadata": {},
   "source": [
    "The sklearn library can do all of this using built in routines.  We add to the work above one more import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-electric",
   "metadata": {},
   "source": [
    "The BernoulliNB function takes the binary feature matrix and does all the computations associated with fitting the naive bayes\n",
    "model.  Let's walk through it. We start with the train_reviews and train_labels lists.  This cell builds the Bernoulli classifier B\n",
    "using the vectorized train_reviews and the train_labels data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=100,binary=True,stop_words='english')\n",
    "V = vectorizer.fit(train_reviews)\n",
    "X = V.transform(train_reviews)\n",
    "train_y = np.array(train_labels)\n",
    "B = BernoulliNB().fit(X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-receipt",
   "metadata": {},
   "source": [
    "This cell uses the fitted vectorizer to compute the data matrix for the test reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = V.transform(test_reviews)\n",
    "test_y = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-supervision",
   "metadata": {},
   "source": [
    "Now we find the predictions using the matrix T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = B.predict(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-wages",
   "metadata": {},
   "source": [
    "The score method allows us to tell how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "B.score(T,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-rolling",
   "metadata": {},
   "source": [
    "The logs of the probabilities P(w|+/-) are stored inside B, and they agree with our computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "B.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "LPminus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "LPplus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-perception",
   "metadata": {},
   "source": [
    "## Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-columbus",
   "metadata": {},
   "source": [
    "Carry out the analysis above using the yelp and imdb data.  You can use the sklearn facilities to make your life easier if you want.\n",
    "You can also try the multinomial classifier to see if it works better.  In that case, you need to remove the binary=True flag\n",
    "from the vectorizer so that it counts frequencies.  Here is an example of how the countvectorizer can compute term frequencies. You can also experiment with the \"stop_words\" flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = vectorizer.fit([\"Here is a sentence\", \"Here is another sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "V.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = V.transform([\"What are the frequencies in this here sentence\", \"I wrote a sentence about this sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "V.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-virgin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "296bc9f5e650645ada33988802a96dbced28fb57d089f124fd617aa4b6545454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
