{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noticed-supervision",
   "metadata": {},
   "source": [
    "# Naive Bayes Sentiment Analysis\n",
    "\n",
    "## Setup\n",
    "\n",
    "In this lab we will take advantage of some parts of the scikit-learn (sklearn) machine learning library to carry out a Naive Bayes\n",
    "sentiment analysis of some amazon product reviews.\n",
    "\n",
    "In addition to our usual libraries (numpy for linear algebra and bokeh for plotting) we load two functions\n",
    "from sklearn.\n",
    "\n",
    "- ```CountVectorizer``` extracts wordcounts from documents\n",
    "- ```train_test_split``` splits our data up into a \"training set\" from which we will derive our probabilities, and \n",
    "    a \"test\" set that we will use to evaluate our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook\n",
    "import numpy as np\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-impression",
   "metadata": {},
   "source": [
    "First we read our data from the file.  Each line of the file consists of a review, followed by a tab character,\n",
    "followed by a \"0\" or \"1\".  We build a list of reviews and a corresponding list of labels from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "labels = []\n",
    "with open(\"amazon.txt\") as f:\n",
    "    for line in f:\n",
    "        review, label = line.strip().split('\\t')\n",
    "        reviews.append(review)\n",
    "        labels.append(int(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-minutes",
   "metadata": {},
   "source": [
    "The train_test_split breaks up the reviews and labels array randomly into two parts; by default, 75% of the data\n",
    "goes into the train set and 25% into the test set, though this is adjustable. Now we set aside the test data\n",
    "and work only with the training data until the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews, test_reviews, train_labels, test_labels = train_test_split(reviews, labels,random_state=11)\n",
    "print('Length of train_reviews is ', len(train_reviews))\n",
    "print('Length of test_reviews is ',len(test_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-trunk",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "Now we use the CountVectorizer function to analyze our reviews.  The syntax here is that we create a CountVectorizer object and apply it to our review data.  We set options to say that  we only want to keep track of the 100 most common words in the reviews and (using ```binary=True```)that we only want to mark words that occur with a zero or 1 -- otherwise the routine will count the number of occurrences of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = CountVectorizer(max_features=100,binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-ocean",
   "metadata": {},
   "source": [
    "To see how this works, let's apply it to a couple of simple sentences. The vectorizer expects a list of sentences, so we'll give a list of three sentences.  It returns a matrix whose rows correspond to the sentences and whose columns are features corresponding to the words it discovered in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = F.fit_transform(['This is a simple sentence that contains the word sentence twice.' ,\n",
    "                          'This is another sentence.',\n",
    "                          'A simple sentence has twice.'])\n",
    "simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-georgia",
   "metadata": {},
   "source": [
    "The vectorizer returns a \"sparse\" array, which is an efficient way to store large matrices which are mostly zero.  We'll\n",
    "see how to view it in a moment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-sensitivity",
   "metadata": {},
   "source": [
    "First, we can ask the vectorizer for the vocabulary that it uncovered.  It returns a python dictionary that associates\n",
    "words to columns.  So for example in this case the word ```word``` corresponds to the 9th column of the data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-highway",
   "metadata": {},
   "source": [
    "Now the array ```simple```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-tuning",
   "metadata": {},
   "source": [
    "Each row is the feature vector to a sentence, and each column corresponds to a  word.  So the first sentence does *not* contain the first key word (```another```) but the second one does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-america",
   "metadata": {},
   "source": [
    "The column sums tell us how often each word occurs (total) in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-transfer",
   "metadata": {},
   "source": [
    "Suppose the first sentence is \"positive\" (labelled with 1) and the other two are negative (labelled with zero). The corresponding target array is Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([[1],[0],[0]])\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-sampling",
   "metadata": {},
   "source": [
    "We can compute the frequencies in the positive documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.transpose() @ simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-mechanism",
   "metadata": {},
   "source": [
    "and in the negative ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1-Y).transpose() @ simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-grass",
   "metadata": {},
   "source": [
    "The fourth word is 'sentence' which does indeed occur once in the type 1 sentences and 2 times in the type 0 ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-woman",
   "metadata": {},
   "source": [
    "The numbers of sentences of each type are $Y^TY$ and $(1-Y)^T(1-Y)$ although numpy thinks these are two dimensional 1x1 arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.transpose()@Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1-Y).transpose()@(1-Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-saturn",
   "metadata": {},
   "source": [
    "We can compute the conditional probabilities with which each word occurs in the two types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pplus = Y.transpose()@simple/(Y.transpose()@Y)\n",
    "Pplus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-basics",
   "metadata": {},
   "source": [
    "Just as a check, the first word, ```another```, occurs only in the second sentence, so if has a 50% chance of\n",
    "occurring in a sentence labelled zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pminus = (1-Y).transpose()@simple/((1-Y).transpose()@(1-Y))\n",
    "Pminus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-think",
   "metadata": {},
   "source": [
    "Now let's look at our training data.  First we use the CountVectorizer to compute the feature matrix.  We're going to add an option \n",
    "to tell the vectorizer to ignore elements of a list of \"stop words\" like he, his, at, him, ... to simplify things. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-thailand",
   "metadata": {},
   "source": [
    "## The product review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=100,binary=True,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = vectorizer.fit_transform(train_reviews).toarray()\n",
    "keywords = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-viewer",
   "metadata": {},
   "source": [
    "Let's compute the frequencies of the 100 words in each of the two classes.  We convert our labels into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-covering",
   "metadata": {},
   "source": [
    "We use our formulae to compute the frequencies and the conditional probabilitiy vectors for the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_plus = (train_y.transpose()@train_matrix)\n",
    "Nplus = train_y.transpose()@train_y\n",
    "Pplus = freq_plus/Nplus\n",
    "freq_minus = ((1-train_y).transpose()@train_matrix)\n",
    "Nminus = ((1-train_y).transpose()@(1-train_y))\n",
    "Pminus = freq_minus/Nminus\n",
    "N = Nminus+Nplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Nplus, Nminus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-traveler",
   "metadata": {},
   "source": [
    "Here we use a trick called \"indirect sort\" to find the words with largest P(w|+) and P(w|-).  Argsort returns this *locations* of\n",
    "the elements in order.  So indices[0] is the location in the Pplus array with the smallest value, indices[1] the next smallest value, and so on.\n",
    "We use these indices to extract the corresponding keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(Pplus)\n",
    "[keywords[i] for i in indices[::-1]][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(Pminus)\n",
    "[keywords[i] for i in indices[::-1]][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a fancy use of bokeh to add hover labels to the dots\n",
    "source = ColumnDataSource({'+':Pplus,'-':Pminus,'word':keywords})\n",
    "f=figure()\n",
    "f.scatter(x='+',y='-',source=source)\n",
    "\n",
    "f.xaxis.axis_label='P(w|+)'\n",
    "f.yaxis.axis_label = 'P(w|-)'\n",
    "f.line(x=[0,.2],y=[0,.2])\n",
    "f.add_tools(HoverTool(tooltips=[(\"word\",\"@word\")]))\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-ensemble",
   "metadata": {},
   "source": [
    "To avoid taking the logarithm of zero, we increase all of the frequency counts by 1, as well as the Nplus and Nminus by 1. This is often called\n",
    "\"smoothing.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_plus = freq_plus+1\n",
    "freq_minus = freq_minus+1\n",
    "Nplus = Nplus+2\n",
    "Nminus = Nminus+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "LPplus = np.log(freq_plus/Nplus)\n",
    "LPNplus = np.log(1-freq_plus/Nplus)\n",
    "LPminus = np.log(freq_minus/Nminus)\n",
    "LPNminus = np.log(1-freq_minus/Nminus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-apache",
   "metadata": {},
   "source": [
    "Using the equation from the notes (which is essentially Bayes rule), we find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "posL = train_matrix @ LPplus + (1-train_matrix) @(1-LPplus) - np.log(Nplus/(N+2))\n",
    "negL  = train_matrix @ LPminus + (1-train_matrix)@(1-LPminus) - np.log(Nminus/(N+2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-dayton",
   "metadata": {},
   "source": [
    "Recall that posL and negL are the likelihoods that a particular review is positive or negative, and our decision criterion is:\n",
    "- label 1 if posL-negL>0\n",
    "- label 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-christianity",
   "metadata": {},
   "source": [
    "Our decision array has a 1 if posL>negL and a zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = (posL > negL).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-incidence",
   "metadata": {},
   "source": [
    "Our check array as a 1 if decision and the original label agree, and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = (decision==train_y).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-malawi",
   "metadata": {},
   "source": [
    "They agree 554/750 times, or about 75% of the time.  That's much better than guessing, which would only be right 50% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-imperial",
   "metadata": {},
   "source": [
    "Finally, we use the test data to see if we can predict labels on \"new\" data.\n",
    "We re-use the LPplus and LPminus parameters, as well as the Nplus/N and Nminus/N from the training data.\n",
    "But we need to compute the data matrix for the test data *based on the features derived from the training data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(train_reviews)\n",
    "test_matrix = vectorizer.transform(test_reviews).toarray()\n",
    "test_y = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "posL = test_matrix @ LPplus + (1-test_matrix)@(LPNplus) -np.log(Nplus/(Nplus+Nminus))\n",
    "negL = test_matrix @ LPminus + (1-test_matrix)@(LPNminus) - np.log(Nminus/(Nplus+Nminus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decision = (posL > negL).astype(int)\n",
    "check = (test_decision == test_y).astype(int)\n",
    "np.sum(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-offer",
   "metadata": {},
   "source": [
    "So we have correctly classified 182/250 reviews from the test set for an accuracy of 171/250 = 73%. Much better than guessing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-cheat",
   "metadata": {},
   "source": [
    "## Using the sklearn facilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-aquarium",
   "metadata": {},
   "source": [
    "The sklearn library can do all of this using built in routines.  We add to the work above one more import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-basis",
   "metadata": {},
   "source": [
    "The BernoulliNB function takes the binary feature matrix and does all the computations associated with fitting the naive bayes\n",
    "model.  Let's walk through it. We start with the train_reviews and train_labels lists.  This cell builds the Bernoulli classifier B\n",
    "using the vectorized train_reviews and the train_labels data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=100,binary=True,stop_words='english')\n",
    "V = vectorizer.fit(train_reviews)\n",
    "X = V.transform(train_reviews)\n",
    "train_y = np.array(train_labels)\n",
    "B = BernoulliNB().fit(X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-seattle",
   "metadata": {},
   "source": [
    "This cell uses the fitted vectorizer to compute the data matrix for the test reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = V.transform(test_reviews)\n",
    "test_y = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-deadline",
   "metadata": {},
   "source": [
    "Now we find the predictions using the matrix T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = B.predict(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-chrome",
   "metadata": {},
   "source": [
    "The score method allows us to tell how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "B.score(T,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-circuit",
   "metadata": {},
   "source": [
    "The logs of the probabilities P(w|+/-) are stored inside B, and they agree with our computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "B.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "LPminus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "LPplus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-craps",
   "metadata": {},
   "source": [
    "## Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-transmission",
   "metadata": {},
   "source": [
    "Carry out the analysis above using the yelp and imdb data.  You can use the sklearn facilities to make your life easier if you want.\n",
    "You can also try the multinomial classifier to see if it works better.  In that case, you need to remove the binary=True flag\n",
    "from the vectorizer so that it counts frequencies.  Here is an example of how the countvectorizer can compute term frequencies. You can also experiment with the \"stop_words\" flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = vectorizer.fit([\"Here is a sentence\", \"Here is another sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "V.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = V.transform([\"What are the frequencies in this here sentence\", \"I wrote a sentence about this sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "V.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-amazon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
